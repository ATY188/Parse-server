"""
ç¶²é å…§å®¹è§£æå™¨ API - Python ç‰ˆæœ¬ï¼ˆå¢å¼·ç‰ˆï¼‰
ä½¿ç”¨ FastAPI + trafilatura
æ”¯æ´é‡è©¦ã€æ›´å¥½çš„ headersã€SSL éŒ¯èª¤è™•ç†

å®‰è£å¥—ä»¶ï¼š
pip install fastapi uvicorn trafilatura httpx python-multipart

å•Ÿå‹•æ–¹å¼ï¼š
python parser-server.py
æˆ–
uvicorn parser-server:app --reload --port 3000
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel, HttpUrl, validator
from typing import Optional, Dict, Any
import trafilatura
import httpx
from datetime import datetime
import uvicorn
import os
import asyncio
import random
import shutil
import glob
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout

# ==================== ä½µç™¼æ§åˆ¶ ====================
# ğŸ”§ ä¿®å¾© BlockingIOError: é™åˆ¶åŒæ™‚é‹è¡Œçš„ Playwright å¯¦ä¾‹æ•¸é‡
# Railway Pro Plan å»ºè­°æœ€å¤š 2-3 å€‹ä¸¦ç™¼å¯¦ä¾‹
PLAYWRIGHT_SEMAPHORE = asyncio.Semaphore(2)

# ==================== /tmp æ¸…ç†åŠŸèƒ½ ====================
def cleanup_chromium_temp():
    """
    æ¸…ç† Chromium/Playwright ç”¢ç”Ÿçš„è‡¨æ™‚æ–‡ä»¶
    é˜²æ­¢ /tmp ç©ºé–“è¢«ä½”æ»¿å°è‡´ç„¡æ³•å•Ÿå‹•æ–°çš„ç€è¦½å™¨
    """
    patterns = [
        '/tmp/.org.chromium.*',
        '/tmp/playwright*',
        '/tmp/.com.google.Chrome.*',
        '/tmp/chromium*',
        '/tmp/.X*-lock',
        '/tmp/core.*',
    ]
    cleaned = 0
    for pattern in patterns:
        for path in glob.glob(pattern):
            try:
                if os.path.isdir(path):
                    shutil.rmtree(path, ignore_errors=True)
                else:
                    os.remove(path)
                cleaned += 1
            except:
                pass
    if cleaned > 0:
        print(f"[Cleanup] ğŸ§¹ å·²æ¸…ç† {cleaned} å€‹è‡¨æ™‚æ–‡ä»¶/ç›®éŒ„")
    return cleaned


# å»ºç«‹ FastAPI æ‡‰ç”¨
app = FastAPI(
    title="ç¶²é å…§å®¹è§£æå™¨ APIï¼ˆå¢å¼·ç‰ˆ + æ™ºæ…§è·¯ç”±ï¼‰",
    description="ä½¿ç”¨ trafilatura è‡ªå‹•æå–ç¶²é æ–‡ç« å…§å®¹ï¼Œæ”¯æ´é‡è©¦å’ŒéŒ¯èª¤è™•ç†ï¼Œæ™ºæ…§è·¯ç”±å„ªåŒ–",
    version="1.8.0"  # ç‰ˆæœ¬å‡ç´š
)

# ==================== CORS é…ç½® ====================
# å…è¨±æ‰€æœ‰ä¾†æºè¨ªå• APIï¼ˆé©ç”¨æ–¼å…¬é–‹ APIï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # å…è¨±æ‰€æœ‰ä¾†æº
    allow_credentials=True,
    allow_methods=["*"],  # å…è¨±æ‰€æœ‰ HTTP æ–¹æ³• (GET, POST, PUT, DELETE, HEAD, OPTIONS)
    allow_headers=["*"],  # å…è¨±æ‰€æœ‰ headers
)

# ==================== æ™ºæ…§è·¯ç”±é…ç½® ====================

# å·²çŸ¥ç„¡æ³•è§£æçš„ç¶²ç«™ï¼ˆé»‘åå–®ï¼‰- ç›´æ¥è¿”å›å¤±æ•—ï¼Œå»ºè­°ä½¿ç”¨ RSS
BLOCKED_DOMAINS = [
    'reuters.com',           # 401 Forbidden - éœ€è¦è¨‚é–±ï¼ˆpaywallï¼‰
    'japantimes.co.jp',      # Cloudflare äººé¡é©—è­‰ï¼ˆCAPTCHAï¼‰
    'content-technology.com', # 403 Forbidden - å¼·åçˆ¬
    'isna.ir',               # åœ°å€å°é–ï¼ˆä¼Šæœ—ï¼‰
    # 'koin.com',            # å·²ç§»é™¤ï¼šçµ¦ Playwright ä¸€æ¬¡æ©Ÿæœƒ
]

# å·²çŸ¥å¿…é ˆä½¿ç”¨å‹•æ…‹æ¸²æŸ“çš„ç¶²ç«™ï¼ˆç›´æ¥ç”¨ Playwrightï¼Œä¸æµªè²»æ™‚é–“è©¦éœæ…‹ï¼‰
DYNAMIC_REQUIRED_DOMAINS = [
    'techstory.in',          # å°åº¦ç§‘æŠ€æ–°èï¼ˆå‹•æ…‹è¼‰å…¥ï¼‰
    'peoplematters.in',      # å°åº¦äººåŠ›è³‡æºæ–°èï¼ˆå‹•æ…‹è¼‰å…¥ï¼‰
    'storm.mg',              # é¢¨å‚³åª’ï¼ˆå‹•æ…‹è¼‰å…¥ï¼‰
    'ustv.com.tw',           # éå‡¡æ–°èï¼ˆå‹•æ…‹è¼‰å…¥ï¼‰
    'designnews.com',        # ç¾åœ‹è¨­è¨ˆæ–°èï¼ˆå‹•æ…‹è¼‰å…¥ï¼‰
    'gurufocus.com',         # ç¾åœ‹é‡‘èæ–°èï¼ˆå‹•æ…‹è¼‰å…¥ï¼‰
    'sammyfans.com',         # ç§‘æŠ€æ–°èï¼ˆå‹•æ…‹è¼‰å…¥ï¼‰
    'manilatimes.net',       # è²å¾‹è³“æ–°èï¼ˆå‹•æ…‹è¼‰å…¥ï¼‰
]

# å·²çŸ¥éœæ…‹è§£æå³å¯çš„ç¶²ç«™ï¼ˆå„ªå…ˆä½¿ç”¨éœæ…‹ï¼Œé€Ÿåº¦å¿«ï¼‰
STATIC_OK_DOMAINS = [
    # å¯ä»¥åœ¨æ¸¬è©¦å¾Œé€æ­¥æ·»åŠ 
    # ä¾‹å¦‚ï¼š'example.com', 'blog.example.com'
]

# AMP é é¢è­¦å‘Šæ¸…å–®ï¼ˆå»ºè­°è½‰æ›ç‚ºé AMP ç‰ˆæœ¬ï¼‰
AMP_WARNING_PATTERNS = [
    '/amp', '/amp/', '?amp=1', '&amp=1', '.amp.html'
]

def extract_domain(url: str) -> str:
    """å¾ URL ä¸­æå–åŸŸå"""
    from urllib.parse import urlparse
    try:
        parsed = urlparse(url)
        return parsed.netloc.lower()
    except:
        return ""

def is_blocked_domain(url: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦ç‚ºé»‘åå–®åŸŸå"""
    domain = extract_domain(url)
    return any(blocked in domain for blocked in BLOCKED_DOMAINS)

def requires_dynamic_rendering(url: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦éœ€è¦å‹•æ…‹æ¸²æŸ“"""
    domain = extract_domain(url)
    return any(dynamic in domain for dynamic in DYNAMIC_REQUIRED_DOMAINS)

def is_static_ok(url: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦å¯ä»¥ä½¿ç”¨éœæ…‹è§£æ"""
    domain = extract_domain(url)
    return any(static in domain for static in STATIC_OK_DOMAINS)

def is_amp_url(url: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦ç‚º AMP é é¢"""
    return any(pattern in url.lower() for pattern in AMP_WARNING_PATTERNS)

def get_routing_decision(url: str) -> Dict[str, Any]:
    """
    æ™ºæ…§è·¯ç”±æ±ºç­–
    
    Returns:
        {
            "action": "block" | "dynamic" | "static" | "try_static_first",
            "reason": "åŸå› èªªæ˜",
            "suggestion": "å»ºè­°ï¼ˆå¦‚æœæœ‰ï¼‰"
        }
    """
    # æª¢æŸ¥é»‘åå–®
    if is_blocked_domain(url):
        return {
            "action": "block",
            "reason": "åŸŸååœ¨é»‘åå–®ä¸­ï¼ˆå·²çŸ¥ç„¡æ³•è§£æï¼‰",
            "suggestion": "å»ºè­°ä½¿ç”¨ RSS æ‘˜è¦ä»£æ›¿"
        }
    
    # æª¢æŸ¥ AMP é é¢
    if is_amp_url(url):
        return {
            "action": "dynamic",  # AMP ä¹Ÿç”¨å‹•æ…‹
            "reason": "æª¢æ¸¬åˆ° AMP é é¢",
            "suggestion": "å»ºè­°è½‰æ›ç‚ºé AMP ç‰ˆæœ¬ä»¥ç²å¾—æ›´å¥½æ•ˆæœ"
        }
    
    # æª¢æŸ¥æ˜¯å¦å·²çŸ¥éœ€è¦å‹•æ…‹æ¸²æŸ“
    if requires_dynamic_rendering(url):
        return {
            "action": "dynamic",
            "reason": "åŸŸåå·²çŸ¥éœ€è¦å‹•æ…‹æ¸²æŸ“ï¼ˆJavaScript è¼‰å…¥å…§å®¹ï¼‰",
            "suggestion": None
        }
    
    # æª¢æŸ¥æ˜¯å¦å·²çŸ¥éœæ…‹å³å¯
    if is_static_ok(url):
        return {
            "action": "static",
            "reason": "åŸŸåå·²çŸ¥å¯ä½¿ç”¨éœæ…‹è§£æï¼ˆé€Ÿåº¦å¿«ï¼‰",
            "suggestion": None
        }
    
    # æœªçŸ¥åŸŸåï¼Œå…ˆå˜—è©¦éœæ…‹
    return {
        "action": "try_static_first",
        "reason": "æœªçŸ¥åŸŸåï¼Œå…ˆå˜—è©¦éœæ…‹è§£æï¼Œå¤±æ•—å¾Œè‡ªå‹•ä½¿ç”¨å‹•æ…‹",
        "suggestion": None
    }

# ==================== æ™ºæ…§è·¯ç”±é…ç½®çµæŸ ====================

# å¤šçµ„ User-Agent è¼ªæµä½¿ç”¨
USER_AGENTS = [
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

def get_random_user_agent():
    """éš¨æ©Ÿé¸æ“‡ User-Agent"""
    return random.choice(USER_AGENTS)

def get_enhanced_headers(url: str):
    """ç²å–å¢å¼·çš„ HTTP headers"""
    from urllib.parse import urlparse
    parsed_url = urlparse(url)
    
    return {
        'User-Agent': get_random_user_agent(),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Referer': f'https://{parsed_url.netloc}/',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0'
    }

# è«‹æ±‚è³‡æ–™æ¨¡å‹
class ParseRequest(BaseModel):
    url: str
    max_retries: Optional[int] = 3
    skip_ssl: Optional[bool] = False
    
    @validator('url')
    def validate_url(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­')
        return v

class ParseDynamicRequest(BaseModel):
    url: str
    wait_for: Optional[str] = None
    block_ads: Optional[bool] = True  # é è¨­å±è”½å»£å‘Š
    stealth_mode: Optional[bool] = True  # é è¨­å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼
    
    @validator('url')
    def validate_url(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­')
        return v

class ParseWebhookRequest(BaseModel):
    url: str
    webhook_url: str
    metadata: Optional[Dict[str, Any]] = {}
    max_retries: Optional[int] = 3
    skip_ssl: Optional[bool] = False
    
    @validator('url', 'webhook_url')
    def validate_urls(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­')
        return v

class DecodeGoogleUrlRequest(BaseModel):
    url: str
    
    @validator('url')
    def validate_url(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­')
        return v


async def fetch_with_playwright(
    url: str, 
    wait_for: Optional[str] = None,
    block_ads: bool = True,
    stealth_mode: bool = True
) -> str:
    """
    ä½¿ç”¨ Playwright ç²å–å‹•æ…‹ç¶²é å…§å®¹ï¼ˆå¢å¼·ç‰ˆï¼‰
    
    Args:
        url: è¦è¨ªå•çš„ç¶²é  URL
        wait_for: ç­‰å¾…ç‰¹å®šå…ƒç´ ï¼ˆCSS selectorï¼‰å‡ºç¾ï¼Œä¾‹å¦‚ 'article' æˆ– '.content'
        block_ads: æ˜¯å¦å±è”½å»£å‘Šï¼ˆé è¨­ Trueï¼‰
        stealth_mode: æ˜¯å¦å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼ï¼ˆé è¨­ Trueï¼‰
        
    Returns:
        æ¸²æŸ“å¾Œçš„ HTML å…§å®¹
        
    Raises:
        Exception: ç•¶ç€è¦½å™¨æ“ä½œå¤±æ•—æ™‚
    """
    # ğŸ”§ ä½¿ç”¨ä¿¡è™Ÿé‡æ§åˆ¶ä½µç™¼ï¼Œé¿å… BlockingIOError
    async with PLAYWRIGHT_SEMAPHORE:
        print(f"[Playwright] ğŸ”’ ç²å–ä½µç™¼é–...")
        
        async with async_playwright() as p:
            browser = None  # åˆå§‹åŒ–è®Šæ•¸ï¼Œç¢ºä¿ finally å¯ä»¥æª¢æŸ¥
            context = None
            try:
                # å•Ÿå‹• Chromium ç€è¦½å™¨ï¼ˆç„¡é ­æ¨¡å¼ï¼‰
                print(f"[Playwright] å•Ÿå‹•ç€è¦½å™¨...")
                browser = await p.chromium.launch(
                    headless=True,
                    args=[
                        # åŸºæœ¬è¨­å®š
                        '--disable-blink-features=AutomationControlled',  # ç¦ç”¨è‡ªå‹•åŒ–æ§åˆ¶ç‰¹å¾µ
                        '--no-sandbox',
                        '--disable-setuid-sandbox',
                        # ğŸ”§ ä¿®å¾© BlockingIOError - è¨˜æ†¶é«”å’Œè³‡æºå„ªåŒ–
                        '--disable-dev-shm-usage',          # ä¸ä½¿ç”¨ /dev/shmï¼ˆé—œéµä¿®å¾©ï¼ï¼‰
                        '--disable-gpu',                     # ç¦ç”¨ GPUï¼ˆå®¹å™¨ç’°å¢ƒï¼‰
                        '--disable-software-rasterizer',     # ç¦ç”¨è»Ÿé«”å…‰æŸµåŒ–
                        '--single-process',                  # å–®é€²ç¨‹æ¨¡å¼ï¼ˆæ¸›å°‘è³‡æºæ¶ˆè€—ï¼‰
                        '--no-zygote',                       # ç¦ç”¨ zygote é€²ç¨‹
                        # è¨˜æ†¶é«”å„ªåŒ–
                        '--disable-extensions',              # ç¦ç”¨æ“´å……
                        '--disable-background-networking',   # ç¦ç”¨èƒŒæ™¯ç¶²è·¯
                        '--disable-sync',                    # ç¦ç”¨åŒæ­¥
                        '--disable-translate',               # ç¦ç”¨ç¿»è­¯
                        '--disable-features=TranslateUI',
                        '--disable-default-apps',            # ç¦ç”¨é è¨­æ‡‰ç”¨
                        '--mute-audio',                      # éœéŸ³
                        '--hide-scrollbars',                 # éš±è—æ»¾å‹•æ¢
                        # ç©©å®šæ€§
                        '--disable-hang-monitor',            # ç¦ç”¨æ›èµ·ç›£æ§
                        '--disable-prompt-on-repost',        # ç¦ç”¨é‡æ–°æäº¤æç¤º
                        '--disable-component-update',        # ç¦ç”¨çµ„ä»¶æ›´æ–°
                        '--ignore-certificate-errors',       # å¿½ç•¥è­‰æ›¸éŒ¯èª¤
                    ]
                )
                
                # å‰µå»ºæ–°çš„ç€è¦½å™¨ä¸Šä¸‹æ–‡ï¼ˆæ¨¡æ“¬çœŸå¯¦ç”¨æˆ¶ï¼‰
                context = await browser.new_context(
                    user_agent=get_random_user_agent(),
                    viewport={'width': 1920, 'height': 1080},
                    locale='zh-TW',
                    timezone_id='Asia/Taipei',
                    color_scheme='light',
                    extra_http_headers={
                        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
                        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                    }
                )
                
                # å¦‚æœå•Ÿç”¨å»£å‘Šå±è”½
                if block_ads:
                    print(f"[Playwright] å•Ÿç”¨å»£å‘Šå±è”½")
                    ad_domains = [
                        'doubleclick.net', 'googlesyndication.com', 'googletagmanager.com',
                        'google-analytics.com', 'facebook.com/tr/', 'scorecardresearch.com',
                        'ad.doubleclick.net', 'static.ads-twitter.com', 'ads.yahoo.com',
                        'pagead2.googlesyndication.com', 'adservice.google.com',
                        'analytics.google.com', 'googleadservices.com'
                    ]
                    
                    await context.route("**/*", lambda route: (
                        route.abort() if any(ad in route.request.url for ad in ad_domains)
                        else route.continue_()
                    ))
                
                # å‰µå»ºæ–°é é¢
                page = await context.new_page()
                
                # å¦‚æœå•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼
                if stealth_mode:
                    print(f"[Playwright] å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼")
                    # éš±è— webdriver ç‰¹å¾µ
                    await page.add_init_script("""
                        // ç§»é™¤ webdriver æ¨™è¨˜
                        Object.defineProperty(navigator, 'webdriver', {
                            get: () => false
                        });
                        
                        // å½è£ Chrome å°è±¡
                        window.chrome = {
                            runtime: {}
                        };
                        
                        // ä¿®æ”¹ permissions
                        const originalQuery = window.navigator.permissions.query;
                        window.navigator.permissions.query = (parameters) => (
                            parameters.name === 'notifications' ?
                                Promise.resolve({ state: Notification.permission }) :
                                originalQuery(parameters)
                        );
                        
                        // å½è£ plugins
                        Object.defineProperty(navigator, 'plugins', {
                            get: () => [1, 2, 3, 4, 5]
                        });
                        
                        // å½è£ languages
                        Object.defineProperty(navigator, 'languages', {
                            get: () => ['zh-TW', 'zh', 'en-US', 'en']
                        });
                    """)
                
                # è¨ªå•ç¶²é ï¼ˆä½¿ç”¨æ›´å¯¬é¬†çš„ç­–ç•¥ä»¥æå‡ç©©å®šæ€§ï¼‰
                print(f"[Playwright] æ­£åœ¨è¨ªå•: {url}")
                await page.goto(url, wait_until='domcontentloaded', timeout=90000)  # 90 ç§’ï¼Œä½¿ç”¨ domcontentloaded ç­–ç•¥
                
                # éš¨æ©Ÿå»¶é²ï¼ˆæ¨¡æ“¬äººé¡è¡Œç‚ºï¼‰
                delay = random.uniform(1, 2.5)
                print(f"[Playwright] éš¨æ©Ÿå»¶é² {delay:.1f} ç§’...")
                await asyncio.sleep(delay)
                
                # ç§»é™¤å»£å‘Šå…ƒç´ ï¼ˆDOM å±¤é¢ï¼‰
                if block_ads:
                    await page.evaluate("""() => {
                        // ç§»é™¤å¸¸è¦‹å»£å‘Šå…ƒç´ 
                        const selectors = [
                            '[class*="ad-"]', '[class*="ad_"]', '[id*="ad-"]', '[id*="ad_"]',
                            '[class*="advertisement"]', '[class*="banner"]',
                            'iframe[src*="ads"]', 'iframe[src*="doubleclick"]',
                            '.ad', '.ads', '#ad', '#ads'
                        ];
                        
                        selectors.forEach(selector => {
                            try {
                                document.querySelectorAll(selector).forEach(el => el.remove());
                            } catch(e) {}
                        });
                    }""")
                
                # å¦‚æœæŒ‡å®šäº†ç­‰å¾…å…ƒç´ ï¼Œç­‰å¾…è©²å…ƒç´ å‡ºç¾
                if wait_for:
                    print(f"[Playwright] ç­‰å¾…å…ƒç´ : {wait_for}")
                    try:
                        await page.wait_for_selector(wait_for, timeout=20000)  # å¢åŠ åˆ° 20 ç§’
                    except:
                        print(f"[Playwright] è­¦å‘Šï¼šå…ƒç´  {wait_for} æœªæ‰¾åˆ°ï¼Œç¹¼çºŒæå–å…§å®¹")
                
                # æ»¾å‹•é é¢ä»¥è§¸ç™¼æ‡¶åŠ è¼‰ï¼ˆå„ªåŒ–ç‰ˆï¼šå¿«é€Ÿåˆ†æ®µæ»¾å‹•ï¼‰
                print(f"[Playwright] æ»¾å‹•é é¢ä»¥è¼‰å…¥å‹•æ…‹å…§å®¹...")
                await page.evaluate("""() => {
                    // å¿«é€Ÿåˆ†æ®µæ»¾å‹•åˆ°é é¢ä¸åŒä½ç½®
                    const positions = [0.3, 0.6, 1.0];  // 30%, 60%, 100%
                    positions.forEach((ratio, index) => {
                        setTimeout(() => {
                            window.scrollTo(0, document.body.scrollHeight * ratio);
                        }, index * 400);  // æ¯ 400ms æ»¾å‹•ä¸€æ¬¡
                    });
                }""")
                
                # å†ç­‰å¾…ä¸€ä¸‹ï¼Œç¢ºä¿å…§å®¹è¼‰å…¥å®Œæˆï¼ˆçµ¦æ‡¶åŠ è¼‰æ›´å¤šæ™‚é–“ï¼‰
                await asyncio.sleep(2)
            
                # ç²å–æ¸²æŸ“å¾Œçš„ HTML
                html_content = await page.content()
                
                print(f"[Playwright] âœ… æˆåŠŸç²å–å…§å®¹ï¼Œé•·åº¦: {len(html_content)}")
                return html_content
                
            except PlaywrightTimeout as e:
                raise Exception(f"Playwright è¶…æ™‚: {str(e)}")
            except Exception as e:
                raise Exception(f"Playwright éŒ¯èª¤: {str(e)}")
            finally:
                # âš ï¸ é‡è¦ï¼šç¢ºä¿ç€è¦½å™¨ä¸€å®šæœƒè¢«é—œé–‰ï¼Œé¿å…è¨˜æ†¶é«”æ´©æ¼
                # ğŸ”§ æŒ‰é †åºé—œé–‰ï¼šå…ˆ contextï¼Œå† browser
                if context:
                    try:
                        await context.close()
                        print(f"[Playwright] ğŸ§¹ Context å·²é—œé–‰")
                    except:
                        pass
                if browser:
                    try:
                        await browser.close()
                        print(f"[Playwright] ğŸ§¹ ç€è¦½å™¨å·²é—œé–‰")
                    except:
                        pass  # å¿½ç•¥é—œé–‰æ™‚çš„éŒ¯èª¤
                print(f"[Playwright] ğŸ”“ é‡‹æ”¾ä½µç™¼é–")
                # ğŸ§¹ æ¸…ç† Chromium è‡¨æ™‚æ–‡ä»¶ï¼Œé˜²æ­¢ /tmp ç©ºé–“è€—ç›¡
                cleanup_chromium_temp()


async def fetch_and_parse_with_playwright(
    url: str,
    wait_for: Optional[str] = None,
    block_ads: bool = True,
    stealth_mode: bool = True,
    max_retries: int = 2
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ Playwright ä¸‹è¼‰ä¸¦è§£æå‹•æ…‹ç¶²é å…§å®¹ï¼ˆå¢å¼·ç‰ˆ + é‡è©¦æ©Ÿåˆ¶ï¼‰
    
    Args:
        url: è¦è§£æçš„ç¶²é  URL
        wait_for: ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç¾
        block_ads: æ˜¯å¦å±è”½å»£å‘Š
        stealth_mode: æ˜¯å¦å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ˆé è¨­ 2 æ¬¡ï¼‰
        
    Returns:
        è§£æå¾Œçš„è³‡æ–™å­—å…¸
    """
    last_error = None
    
    for attempt in range(1, max_retries + 1):
        try:
            if attempt > 1:
                print(f"[Playwright] é‡è©¦ {attempt}/{max_retries}")
                await asyncio.sleep(3)  # ç­‰å¾… 3 ç§’å¾Œé‡è©¦
            
            # ä½¿ç”¨ Playwright ç²å–æ¸²æŸ“å¾Œçš„ HTML
            html_content = await fetch_with_playwright(url, wait_for, block_ads, stealth_mode)
            
            # ä½¿ç”¨ trafilatura è§£æå…§å®¹
            try:
                text_content = trafilatura.extract(
                    html_content,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False
                )
            except Exception as e:
                print(f"[è­¦å‘Š] trafilatura.extract å¤±æ•—: {e}")
                text_content = None
            
            # æå–å…ƒæ•¸æ“š
            try:
                metadata = trafilatura.extract_metadata(html_content)
            except Exception as e:
                print(f"[è­¦å‘Š] trafilatura.extract_metadata å¤±æ•—: {e}")
                metadata = None
            
            # æå– XML æ ¼å¼å…§å®¹
            try:
                html_formatted = trafilatura.extract(
                    html_content,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False,
                    output_format='xml'
                )
            except Exception as e:
                print(f"[è­¦å‘Š] trafilatura.extract (XML) å¤±æ•—: {e}")
                html_formatted = None
            
            # æ•´ç†å›å‚³è³‡æ–™
            parsed_data = {
                "title": getattr(metadata, 'title', None) if metadata else None,
                "author": getattr(metadata, 'author', None) if metadata else None,
                "date_published": getattr(metadata, 'date', None) if metadata else None,
                "url": getattr(metadata, 'url', url) if metadata else url,
                "domain": getattr(metadata, 'sitename', None) if metadata else None,
                "description": getattr(metadata, 'description', None) if metadata else None,
                "categories": getattr(metadata, 'categories', None) if metadata else None,
                "tags": getattr(metadata, 'tags', None) if metadata else None,
                "content": html_formatted or text_content,
                "text_content": text_content,
                "excerpt": text_content[:200] + "..." if text_content and len(text_content) > 200 else text_content,
                "word_count": len(text_content.split()) if text_content else 0,
                "language": getattr(metadata, 'language', None) if metadata else None,
                "rendering_method": "playwright"
            }
            
            # æˆåŠŸè§£æï¼Œè¿”å›çµæœ
            print(f"[Playwright] âœ… ç¬¬ {attempt} æ¬¡å˜—è©¦æˆåŠŸ")
            return {
                "success": True,
                "data": parsed_data,
                "method": "playwright",
                "attempts": attempt
            }
            
        except Exception as e:
            last_error = e
            print(f"[Playwright] âŒ ç¬¬ {attempt} æ¬¡å˜—è©¦å¤±æ•—: {str(e)}")
            
            if attempt == max_retries:
                # æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—äº†
                raise HTTPException(
                    status_code=500,
                    detail=f"ä½¿ç”¨ Playwright è§£æå¤±æ•—ï¼ˆå·²é‡è©¦ {max_retries} æ¬¡ï¼‰: {str(e)}"
                )
    
    # ç†è«–ä¸Šä¸æœƒåˆ°é”é€™è£¡
    raise HTTPException(
        status_code=500,
        detail=f"ä½¿ç”¨ Playwright è§£æå¤±æ•—: {str(last_error)}"
    )


def decode_google_url(google_url: str) -> Optional[str]:
    """
    å¾ Google é‡å®šå‘ URL ä¸­æå–çœŸå¯¦çš„ç›®æ¨™ URL
    
    æ”¯æ´çš„æ ¼å¼ï¼š
    - Google News/Alerts: https://www.google.com/url?url=...
    - Google RSS: https://news.google.com/rss/articles/...
    
    Args:
        google_url: Google é‡å®šå‘ URL
        
    Returns:
        çœŸå¯¦çš„ç›®æ¨™ URLï¼Œå¦‚æœè§£æå¤±æ•—å‰‡è¿”å› None
        
    Examples:
        >>> decode_google_url('https://www.google.com/url?url=https://example.com&...')
        'https://example.com'
    """
    from urllib.parse import urlparse, parse_qs, unquote
    
    try:
        # è§£æ URL
        parsed = urlparse(google_url)
        
        # æª¢æŸ¥æ˜¯å¦ç‚º Google URL
        if 'google.com' not in parsed.netloc:
            # ä¸æ˜¯ Google URLï¼Œç›´æ¥è¿”å›åŸ URL
            return google_url
        
        # è§£ææŸ¥è©¢åƒæ•¸
        query_params = parse_qs(parsed.query)
        
        # å˜—è©¦å¾ä¸åŒçš„åƒæ•¸ä¸­æå– URL
        # å¸¸è¦‹åƒæ•¸ï¼šurl, q, u
        for param in ['url', 'q', 'u']:
            if param in query_params and query_params[param]:
                decoded_url = unquote(query_params[param][0])
                # ç¢ºä¿æ˜¯å®Œæ•´çš„ URL
                if decoded_url.startswith(('http://', 'https://')):
                    return decoded_url
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œè¿”å›åŸ URL
        return google_url
        
    except Exception as e:
        # è§£æå¤±æ•—ï¼Œè¿”å›åŸ URL
        return google_url


# é¦–é è·¯ç”±
@app.get("/")
@app.head("/")  # æ”¯æŒ HEAD è«‹æ±‚ï¼ˆç”¨æ–¼å¥åº·æª¢æŸ¥ï¼‰
async def root():
    """API é¦–é  - é¡¯ç¤ºå¯ç”¨ç«¯é»"""
    return {
        "message": "æ­¡è¿ä½¿ç”¨ç¶²é å…§å®¹è§£æå™¨ API (Python å¢å¼·ç‰ˆ + æ™ºæ…§è·¯ç”±)",
        "framework": "FastAPI + trafilatura + Playwright",
        "version": "1.7.0",
        "features": [
            "ğŸ§  æ™ºæ…§è·¯ç”±ï¼ˆæ ¹æ“šåŸŸåè‡ªå‹•é¸æ“‡æœ€ä½³è§£ææ–¹å¼ï¼‰",
            "â›” é»‘åå–®æ©Ÿåˆ¶ï¼ˆè·³éå·²çŸ¥ç„¡æ³•è§£æçš„ç¶²ç«™ï¼Œç¯€çœæ™‚é–“ï¼‰",
            "ğŸ­ å‹•æ…‹ç¶²ç«™å¿«é€Ÿé€šé“ï¼ˆå·²çŸ¥å‹•æ…‹ç¶²ç«™ç›´æ¥ç”¨ Playwrightï¼‰",
            "ğŸ”„ è‡ªå‹•é™ç´šï¼ˆéœæ…‹å¤±æ•—è‡ªå‹•åˆ‡æ›åˆ°å‹•æ…‹ï¼‰",
            "ğŸ”„ è‡ªå‹•é‡è©¦æ©Ÿåˆ¶ï¼ˆè™•ç† 403/429 éŒ¯èª¤ï¼‰",
            "ğŸ² éš¨æ©Ÿ User-Agent",
            "ğŸ“¡ å¢å¼·çš„ HTTP headers",
            "ğŸ”’ SSL éŒ¯èª¤è™•ç†",
            "â±ï¸ æŒ‡æ•¸é€€é¿ï¼ˆExponential Backoffï¼‰",
            "ğŸš€ Playwright æ”¯æ´ï¼ˆè™•ç†å‹•æ…‹ JavaScript ç¶²ç«™ï¼‰",
            "ğŸš« å»£å‘Šå±è”½ï¼ˆNetwork å’Œ DOM å±¤é¢ï¼‰",
            "ğŸ¥· åçˆ¬èŸ²æ¨¡å¼ï¼ˆéš±è— webdriver ç‰¹å¾µï¼‰",
            "ğŸ“œ è‡ªå‹•æ»¾å‹•è¼‰å…¥æ‡¶åŠ è¼‰å…§å®¹",
            "ğŸ”’ ä½µç™¼æ§åˆ¶ï¼ˆé™åˆ¶åŒæ™‚é‹è¡Œçš„ç€è¦½å™¨æ•¸é‡ï¼Œé¿å…è³‡æºè€—ç›¡ï¼‰",
            "ğŸ›¡ï¸ å®¹å™¨å„ªåŒ–ï¼ˆä¿®å¾© BlockingIOErrorï¼Œç¦ç”¨ /dev/shm ä¾è³´ï¼‰"
        ],
        "smartRouting": {
            "description": "æ™ºæ…§è·¯ç”±æ ¹æ“šåŸŸåæ­·å²è¡¨ç¾è‡ªå‹•é¸æ“‡æœ€ä½³è§£æç­–ç•¥",
            "strategies": {
                "block": "é»‘åå–®åŸŸåï¼ˆreuters.com, japantimes.co.jp ç­‰ï¼‰ç›´æ¥è¿”å›å¤±æ•—ï¼Œå»ºè­°ä½¿ç”¨ RSS",
                "dynamic_direct": "å·²çŸ¥å‹•æ…‹ç¶²ç«™ï¼ˆstorm.mg, techstory.in ç­‰ï¼‰ç›´æ¥ä½¿ç”¨ Playwright",
                "static_only": "å·²çŸ¥éœæ…‹ç¶²ç«™å„ªå…ˆä½¿ç”¨å¿«é€Ÿéœæ…‹è§£æ",
                "fallback": "æœªçŸ¥ç¶²ç«™å…ˆè©¦éœæ…‹ï¼Œå¤±æ•—å¾Œè‡ªå‹•åˆ‡æ›åˆ° Playwright"
            },
            "benefits": [
                "âš¡ æ•ˆèƒ½æå‡ 40-60%ï¼ˆè·³éç„¡æ•ˆå˜—è©¦ï¼‰",
                "ğŸ’° é™ä½è³‡æºæ¶ˆè€—ï¼ˆé¿å…ç„¡è¬‚çš„ Playwright å•Ÿå‹•ï¼‰",
                "ğŸ¯ æ›´é«˜æˆåŠŸç‡ï¼ˆå·²çŸ¥å‹•æ…‹ç¶²ç«™ç›´æ¥ç”¨å°çš„æ–¹æ³•ï¼‰"
            ]
        },
        "endpoints": {
            "parse": {
                "method": "POST",
                "path": "/api/parse",
                "body": {
                    "url": "è¦è§£æçš„ç¶²é  URL",
                    "max_retries": "(é¸å¡«) æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œé è¨­ 3",
                    "skip_ssl": "(é¸å¡«) è·³é SSL é©—è­‰ï¼Œé è¨­ false"
                },
                "description": "è§£ææŒ‡å®š URL çš„ç¶²é å…§å®¹ï¼ˆåŒæ­¥å›å‚³ï¼Œæ”¯æ´é‡è©¦ï¼‰"
            },
            "parseGet": {
                "method": "GET",
                "path": "/api/parse?url=YOUR_URL",
                "description": "ä½¿ç”¨ GET æ–¹æ³•è§£æç¶²é å…§å®¹"
            },
            "parseDynamic": {
                "method": "POST",
                "path": "/api/parse-dynamic",
                "body": {
                    "url": "è¦è§£æçš„ç¶²é  URL",
                    "wait_for": "(é¸å¡«) ç­‰å¾…ç‰¹å®š CSS é¸æ“‡å™¨ï¼Œä¾‹å¦‚ 'article' æˆ– '.content'",
                    "block_ads": "(é¸å¡«) æ˜¯å¦å±è”½å»£å‘Šï¼Œé è¨­ true",
                    "stealth_mode": "(é¸å¡«) æ˜¯å¦å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼ï¼Œé è¨­ true"
                },
                "description": "ä½¿ç”¨ Playwright è§£æå‹•æ…‹ç¶²ç«™ï¼ˆæ”¯æ´ JavaScript æ¸²æŸ“ã€å»£å‘Šå±è”½ã€åçˆ¬èŸ²ï¼‰â­ æ¨è–¦ç”¨æ–¼ SPA ç¶²ç«™å’Œæœ‰åçˆ¬èŸ²çš„ç¶²ç«™"
            },
            "parseWebhook": {
                "method": "POST",
                "path": "/api/parse-webhook",
                "body": {
                    "url": "è¦è§£æçš„ç¶²é  URL",
                    "webhook_url": "n8n webhook URL",
                    "metadata": "(é¸å¡«) é¡å¤–è³‡æ–™",
                    "max_retries": "(é¸å¡«) æœ€å¤§é‡è©¦æ¬¡æ•¸",
                    "skip_ssl": "(é¸å¡«) è·³é SSL é©—è­‰"
                },
                "description": "è§£æç¶²é ä¸¦å›èª¿ webhookï¼ˆé©ç”¨æ–¼ n8n æ•´åˆï¼‰"
            },
            "decodeGoogleUrl": {
                "method": "POST",
                "path": "/api/decode-google-url",
                "body": {
                    "url": "Google é‡å®šå‘ URLï¼ˆä¾‹å¦‚ï¼šhttps://www.google.com/url?url=...ï¼‰"
                },
                "description": "å¾ Google é‡å®šå‘ URL ä¸­æå–çœŸå¯¦çš„ç›®æ¨™ URL â­ é©ç”¨æ–¼ Google Alert/RSS"
            },
            "decodeGoogleUrlGet": {
                "method": "GET",
                "path": "/api/decode-google-url?url=YOUR_GOOGLE_URL",
                "description": "ä½¿ç”¨ GET æ–¹æ³•è§£ç¢¼ Google URL"
            },
            "docs": {
                "method": "GET",
                "path": "/docs",
                "description": "Swagger UI äº’å‹•å¼ API æ–‡ä»¶"
            }
        },
        "examples": [
            'POST /api/parse with body: {"url": "https://example.com/article", "max_retries": 3}',
            'GET /api/parse?url=https://example.com/article',
            'POST /api/parse-webhook with body: {"url": "https://example.com/article", "webhook_url": "https://your-n8n.com/webhook/..."}',
            'POST /api/decode-google-url with body: {"url": "https://www.google.com/url?url=https://example.com/article&..."}',
            'GET /api/decode-google-url?url=https://www.google.com/url?url=https://example.com/article'
        ],
        "errorHandling": {
            "403 Forbidden": "è‡ªå‹•é‡è©¦ + éš¨æ©Ÿ User-Agent + Referer header",
            "429 Too Many Requests": "æŒ‡æ•¸é€€é¿é‡è©¦ï¼ˆ2s, 4s, 8s...ï¼‰",
            "SSL Certificate Error": "å¯é¸æ“‡è·³é SSL é©—è­‰ï¼ˆskip_ssl: trueï¼‰"
        },
        "documentation": "è¨ªå• /docs æŸ¥çœ‹å®Œæ•´ API æ–‡ä»¶"
    }


async def fetch_and_parse_with_retry(
    url: str, 
    max_retries: int = 3, 
    skip_ssl: bool = False
) -> Dict[str, Any]:
    """
    ä¸‹è¼‰ä¸¦è§£æç¶²é å…§å®¹ï¼ˆæ”¯æ´é‡è©¦ï¼‰
    
    Args:
        url: è¦è§£æçš„ç¶²é  URL
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        skip_ssl: æ˜¯å¦è·³é SSL é©—è­‰
        
    Returns:
        è§£æå¾Œçš„è³‡æ–™å­—å…¸
        
    Raises:
        HTTPException: ç•¶ä¸‹è¼‰æˆ–è§£æå¤±æ•—æ™‚
    """
    last_error = None
    
    for attempt in range(1, max_retries + 1):
        try:
            print(f"[å˜—è©¦ {attempt}/{max_retries}] è§£æ: {url}")
            
            # ç²å–å¢å¼·çš„ headers
            headers = get_enhanced_headers(url)
            
            # è¨­å®š timeout å’Œ SSL é©—è­‰
            timeout = httpx.Timeout(30.0, connect=10.0)
            verify_ssl = not skip_ssl
            
            # ä¸‹è¼‰ç¶²é å…§å®¹
            async with httpx.AsyncClient(
                timeout=timeout,
                verify=verify_ssl,
                follow_redirects=True,
                headers=headers
            ) as client:
                response = await client.get(url)
                response.raise_for_status()
                html_content = response.text
            
            # ä½¿ç”¨ trafilatura è§£æå…§å®¹
            try:
                text_content = trafilatura.extract(
                    html_content,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False
                )
            except Exception as e:
                print(f"[è­¦å‘Š] trafilatura.extract å¤±æ•—: {e}")
                text_content = None
            
            # æå–å®Œæ•´è³‡è¨Šï¼ˆåŒ…å«å…ƒæ•¸æ“šï¼‰
            try:
                metadata = trafilatura.extract_metadata(html_content)
            except Exception as e:
                print(f"[è­¦å‘Š] trafilatura.extract_metadata å¤±æ•—: {e}")
                metadata = None
            
            # æå– HTML æ ¼å¼çš„å…§å®¹
            try:
                html_formatted = trafilatura.extract(
                    html_content,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False,
                    output_format='xml'
                )
            except Exception as e:
                print(f"[è­¦å‘Š] trafilatura.extract (XML) å¤±æ•—: {e}")
                html_formatted = None
            
            # æ•´ç†å›å‚³è³‡æ–™
            parsed_data = {
                "title": getattr(metadata, 'title', None) if metadata else None,
                "author": getattr(metadata, 'author', None) if metadata else None,
                "date_published": getattr(metadata, 'date', None) if metadata else None,
                "url": getattr(metadata, 'url', url) if metadata else url,
                "domain": getattr(metadata, 'sitename', None) if metadata else None,
                "description": getattr(metadata, 'description', None) if metadata else None,
                "categories": getattr(metadata, 'categories', None) if metadata else None,
                "tags": getattr(metadata, 'tags', None) if metadata else None,
                "content": html_formatted or text_content,
                "text_content": text_content,
                "excerpt": text_content[:200] + "..." if text_content and len(text_content) > 200 else text_content,
                "word_count": len(text_content.split()) if text_content else 0,
                "language": getattr(metadata, 'language', None) if metadata else None
            }
            
            title_preview = parsed_data.get('title') or 'No title'
            print(f"[æˆåŠŸ] å˜—è©¦ {attempt}: {title_preview[:50] if title_preview else 'No title'}")
            return {
                "success": True,
                "data": parsed_data,
                "attempt": attempt,
                "retries": attempt - 1
            }
            
        except httpx.HTTPStatusError as e:
            last_error = e
            status_code = e.response.status_code
            print(f"[å¤±æ•—] å˜—è©¦ {attempt}: HTTP {status_code} - {str(e)}")
            
            # å¦‚æœæ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œæ‹‹å‡ºéŒ¯èª¤
            if attempt == max_retries:
                raise HTTPException(
                    status_code=500,
                    detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: Client error '{status_code} {e.response.reason_phrase}' for url '{url}'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/{status_code}"
                )
            
            # æ ¹æ“šéŒ¯èª¤é¡å‹æ±ºå®šç­‰å¾…æ™‚é–“
            if status_code in [429, 403]:
                # 429 Too Many Requests æˆ– 403 Forbiddenï¼šæŒ‡æ•¸é€€é¿
                wait_time = (2 ** attempt)  # 2ç§’ã€4ç§’ã€8ç§’...
                print(f"[ç­‰å¾…] {wait_time} ç§’å¾Œé‡è©¦ï¼ˆHTTP {status_code}ï¼‰...")
                await asyncio.sleep(wait_time)
            else:
                # å…¶ä»–éŒ¯èª¤ï¼šçŸ­æš«ç­‰å¾…
                await asyncio.sleep(1)
                
        except httpx.ConnectError as e:
            last_error = e
            print(f"[å¤±æ•—] å˜—è©¦ {attempt}: é€£æ¥éŒ¯èª¤ - {str(e)}")
            
            if attempt == max_retries:
                raise HTTPException(
                    status_code=500,
                    detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: ç„¡æ³•é€£æ¥åˆ° {url}"
                )
            
            await asyncio.sleep(2)
            
        except Exception as e:
            error_msg = str(e)
            print(f"[å¤±æ•—] å˜—è©¦ {attempt}: {error_msg}")
            
            # SSL éŒ¯èª¤è™•ç†
            if "SSL" in error_msg or "certificate" in error_msg.lower():
                last_error = e
                
                if attempt == max_retries:
                    if skip_ssl:
                        raise HTTPException(
                            status_code=500,
                            detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: {error_msg}"
                        )
                    else:
                        raise HTTPException(
                            status_code=500,
                            detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: {error_msg}\n\nğŸ’¡ æç¤ºï¼šå¯ä»¥å˜—è©¦è¨­å®š skip_ssl: true ä¾†è·³é SSL é©—è­‰"
                        )
                
                # ä¸‹æ¬¡å˜—è©¦æ™‚è·³é SSL é©—è­‰
                if not skip_ssl:
                    print(f"[SSL éŒ¯èª¤] ä¸‹æ¬¡å°‡è·³é SSL é©—è­‰...")
                    skip_ssl = True
                    
                await asyncio.sleep(1)
            else:
                # å…¶ä»–éŒ¯èª¤
                last_error = e
                
                if attempt == max_retries:
                    raise HTTPException(
                        status_code=500,
                        detail=f"è§£æç¶²é å¤±æ•—: {error_msg}"
                    )
                
                await asyncio.sleep(1)
    
    # ç†è«–ä¸Šä¸æœƒåˆ°é”é€™è£¡ï¼Œä½†ä»¥é˜²è¬ä¸€
    raise HTTPException(
        status_code=500,
        detail=f"è§£æç¶²é å¤±æ•—: {str(last_error)}"
    )


@app.post("/api/parse")
async def parse_url(request: ParseRequest):
    """
    POST æ–¹æ³•ï¼šè§£æç¶²é å…§å®¹ï¼ˆæ”¯æ´é‡è©¦ + æ™ºæ…§è·¯ç”±ï¼‰
    
    æ™ºæ…§è·¯ç”±æœƒæ ¹æ“šåŸŸåè‡ªå‹•é¸æ“‡æœ€ä½³è§£ææ–¹å¼ï¼š
    - é»‘åå–®åŸŸåï¼šç›´æ¥è¿”å›å¤±æ•—ï¼Œå»ºè­°ä½¿ç”¨ RSS
    - å·²çŸ¥å‹•æ…‹ç¶²ç«™ï¼šç›´æ¥ä½¿ç”¨ Playwrightï¼ˆä¸æµªè²»æ™‚é–“ï¼‰
    - å·²çŸ¥éœæ…‹ç¶²ç«™ï¼šåªç”¨éœæ…‹è§£æï¼ˆé€Ÿåº¦å¿«ï¼‰
    - æœªçŸ¥ç¶²ç«™ï¼šå…ˆè©¦éœæ…‹ï¼Œå¤±æ•—å¾Œè‡ªå‹•ä½¿ç”¨ Playwright
    
    Args:
        request: åŒ…å« urlã€max_retries å’Œ skip_ssl çš„è«‹æ±‚ç‰©ä»¶
        
    Returns:
        è§£æå¾Œçš„ç¶²é å…§å®¹
    """
    print(f"æ­£åœ¨è§£æ: {request.url} (max_retries: {request.max_retries}, skip_ssl: {request.skip_ssl})")
    
    try:
        # ğŸ§  æ™ºæ…§è·¯ç”±æ±ºç­–
        routing = get_routing_decision(request.url)
        print(f"[æ™ºæ…§è·¯ç”±] æ±ºç­–: {routing['action']} - {routing['reason']}")
        
        # æƒ…æ³ 1ï¼šé»‘åå–®åŸŸå - ç›´æ¥è¿”å›å¤±æ•—
        if routing['action'] == 'block':
            print(f"[æ™ºæ…§è·¯ç”±] â›” åŸŸååœ¨é»‘åå–®ä¸­ï¼Œè·³éè§£æ")
            return {
                "success": False,
                "data": None,
                "reason": routing['reason'],
                "suggestion": routing['suggestion'],
                "routing_decision": routing['action'],
                "use_rss_instead": True
            }
        
        # æƒ…æ³ 2ï¼šå·²çŸ¥éœ€è¦å‹•æ…‹æ¸²æŸ“ - ç›´æ¥ç”¨ Playwright
        elif routing['action'] == 'dynamic':
            print(f"[æ™ºæ…§è·¯ç”±] ğŸ­ ç›´æ¥ä½¿ç”¨ Playwrightï¼ˆå·²çŸ¥å‹•æ…‹ç¶²ç«™ï¼‰")
            result = await fetch_and_parse_with_playwright(
                request.url,
                wait_for=None,
                block_ads=True,
                stealth_mode=True
            )
            result['routing_decision'] = 'dynamic_direct'
            if routing.get('suggestion'):
                result['suggestion'] = routing['suggestion']
            return result
        
        # æƒ…æ³ 3ï¼šå·²çŸ¥éœæ…‹å³å¯ - åªç”¨éœæ…‹
        elif routing['action'] == 'static':
            print(f"[æ™ºæ…§è·¯ç”±] âš¡ ä½¿ç”¨éœæ…‹è§£æï¼ˆå·²çŸ¥éœæ…‹ç¶²ç«™ï¼‰")
            result = await fetch_and_parse_with_retry(
                request.url,
                max_retries=request.max_retries,
                skip_ssl=request.skip_ssl
            )
            result['routing_decision'] = 'static_only'
            return result
        
        # æƒ…æ³ 4ï¼šæœªçŸ¥åŸŸå - å…ˆè©¦éœæ…‹ï¼Œå¤±æ•—å¾Œè‡ªå‹•ç”¨ Playwright
        else:  # 'try_static_first'
            print(f"[æ™ºæ…§è·¯ç”±] ğŸ”„ å…ˆè©¦éœæ…‹ï¼Œå¤±æ•—å¾Œè‡ªå‹•ä½¿ç”¨ Playwright")
            
            # å…ˆå˜—è©¦éœæ…‹è§£æ
            try:
                result = await fetch_and_parse_with_retry(
                    request.url,
                    max_retries=1,  # éœæ…‹åªè©¦ä¸€æ¬¡ï¼Œé¿å…æµªè²»æ™‚é–“
                    skip_ssl=request.skip_ssl
                )
                
                # æª¢æŸ¥æ˜¯å¦çœŸçš„æœ‰å…§å®¹
                if result.get('success') and result.get('data', {}).get('text_content'):
                    print(f"[æ™ºæ…§è·¯ç”±] âœ… éœæ…‹è§£ææˆåŠŸ")
                    result['routing_decision'] = 'static_success'
                    return result
                else:
                    raise Exception("éœæ…‹è§£æç„¡å…§å®¹ï¼Œå˜—è©¦å‹•æ…‹æ¸²æŸ“")
                    
            except Exception as static_error:
                print(f"[æ™ºæ…§è·¯ç”±] âš ï¸ éœæ…‹è§£æå¤±æ•—: {str(static_error)}")
                print(f"[æ™ºæ…§è·¯ç”±] ğŸ­ è‡ªå‹•åˆ‡æ›åˆ° Playwright...")
                
                # åˆ‡æ›åˆ° Playwright
                result = await fetch_and_parse_with_playwright(
                    request.url,
                    wait_for=None,
                    block_ads=True,
                    stealth_mode=True
                )
                result['routing_decision'] = 'fallback_to_dynamic'
                result['static_error'] = str(static_error)[:100]  # è¨˜éŒ„éœæ…‹å¤±æ•—åŸå› 
                return result
        
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"è§£æéŒ¯èª¤: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"è§£æç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


@app.get("/api/parse")
async def parse_url_get(url: str, max_retries: int = 3, skip_ssl: bool = False):
    """
    GET æ–¹æ³•ï¼šè§£æç¶²é å…§å®¹ï¼ˆé€é query stringï¼‰
    
    Args:
        url: è¦è§£æçš„ç¶²é  URL
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ˆé è¨­ 3ï¼‰
        skip_ssl: æ˜¯å¦è·³é SSL é©—è­‰ï¼ˆé è¨­ Falseï¼‰
        
    Returns:
        è§£æå¾Œçš„ç¶²é å…§å®¹
    """
    if not url:
        raise HTTPException(
            status_code=400,
            detail="è«‹åœ¨ URL åƒæ•¸ä¸­æä¾›è¦è§£æçš„ç¶²å€"
        )
    
    print(f"æ­£åœ¨è§£æ (GET): {url}")
    
    try:
        result = await fetch_and_parse_with_retry(
            url,
            max_retries=max_retries,
            skip_ssl=skip_ssl
        )
        
        return result
        
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"è§£æéŒ¯èª¤: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"è§£æç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


@app.post("/api/parse-dynamic")
async def parse_url_dynamic(request: ParseDynamicRequest):
    """
    POST æ–¹æ³•ï¼šä½¿ç”¨ Playwright è§£æå‹•æ…‹ç¶²ç«™ï¼ˆå¢å¼·ç‰ˆï¼šæ”¯æ´å»£å‘Šå±è”½å’Œåçˆ¬èŸ²ï¼‰
    
    é©ç”¨æ–¼ï¼š
    - React/Vue/Angular ç­‰å–®é æ‡‰ç”¨ï¼ˆSPAï¼‰
    - JavaScript å‹•æ…‹è¼‰å…¥å…§å®¹çš„ç¶²ç«™
    - éœ€è¦ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç¾çš„ç¶²ç«™
    - æœ‰å»£å‘Šå¹²æ“¾çš„ç¶²ç«™
    - æœ‰åçˆ¬èŸ²æ©Ÿåˆ¶çš„ç¶²ç«™
    
    Args:
        request: åŒ…å«ä»¥ä¸‹æ¬„ä½çš„è«‹æ±‚ç‰©ä»¶
            - url: è¦è§£æçš„ç¶²é  URL
            - wait_for: (é¸å¡«) ç­‰å¾…ç‰¹å®š CSS é¸æ“‡å™¨
            - block_ads: (é¸å¡«) æ˜¯å¦å±è”½å»£å‘Šï¼Œé è¨­ True
            - stealth_mode: (é¸å¡«) æ˜¯å¦å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼ï¼Œé è¨­ True
        
    Returns:
        è§£æå¾Œçš„ç¶²é å…§å®¹
        
    Example:
        POST /api/parse-dynamic
        {
            "url": "https://applealmond.com/posts/296254",
            "wait_for": ".post-content",
            "block_ads": true,
            "stealth_mode": true
        }
    """
    print(f"æ­£åœ¨ä½¿ç”¨ Playwright è§£æ: {request.url}")
    print(f"å»£å‘Šå±è”½: {request.block_ads}, åçˆ¬èŸ²æ¨¡å¼: {request.stealth_mode}")
    if request.wait_for:
        print(f"ç­‰å¾…å…ƒç´ : {request.wait_for}")
    
    try:
        result = await fetch_and_parse_with_playwright(
            request.url, 
            request.wait_for,
            request.block_ads,
            request.stealth_mode
        )
        return result
        
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"Playwright è§£æéŒ¯èª¤: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ä½¿ç”¨ Playwright è§£æç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


async def process_and_webhook(
    url: str, 
    webhook_url: str, 
    metadata: Dict[str, Any],
    max_retries: int = 3,
    skip_ssl: bool = False
):
    """
    èƒŒæ™¯ä»»å‹™ï¼šè§£æç¶²é ä¸¦å›èª¿ webhook
    
    Args:
        url: è¦è§£æçš„ç¶²é  URL
        webhook_url: webhook å›èª¿ URL
        metadata: é¡å¤–çš„å…ƒæ•¸æ“š
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        skip_ssl: æ˜¯å¦è·³é SSL é©—è­‰
    """
    print(f"æ­£åœ¨è§£æ (webhook æ¨¡å¼): {url}")
    
    try:
        # è§£æç¶²é ï¼ˆä½¿ç”¨é‡è©¦æ©Ÿåˆ¶ï¼‰
        result = await fetch_and_parse_with_retry(url, max_retries, skip_ssl)
        
        # æº–å‚™å›èª¿è³‡æ–™
        webhook_data = {
            "success": True,
            "original_url": url,
            "metadata": metadata,
            "parsed_data": result.get("data"),
            "attempt": result.get("attempt"),
            "retries": result.get("retries"),
            "parsed_at": datetime.now().isoformat()
        }
        
        # å›èª¿ webhook
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                webhook_url,
                json=webhook_data
            )
            
            if response.status_code == 200:
                print(f"âœ… Webhook å›èª¿æˆåŠŸ: {webhook_url}")
            else:
                print(f"âŒ Webhook å›èª¿å¤±æ•— ({response.status_code}): {webhook_url}")
                
    except Exception as e:
        print(f"è§£ææˆ–å›èª¿éŒ¯èª¤: {str(e)}")
        
        # å˜—è©¦å›èª¿éŒ¯èª¤è¨Šæ¯
        try:
            error_data = {
                "success": False,
                "original_url": url,
                "metadata": metadata,
                "error": str(e),
                "failed_at": datetime.now().isoformat()
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                await client.post(webhook_url, json=error_data)
                
        except Exception as webhook_error:
            print(f"ç„¡æ³•å›èª¿éŒ¯èª¤è¨Šæ¯: {str(webhook_error)}")


@app.post("/api/parse-webhook")
async def parse_url_webhook(request: ParseWebhookRequest, background_tasks: BackgroundTasks):
    """
    POST æ–¹æ³•ï¼šè§£æç¶²é ä¸¦å›èª¿ webhookï¼ˆç”¨æ–¼ n8n æ•´åˆï¼‰
    
    Args:
        request: åŒ…å« urlã€webhook_urlã€metadataã€max_retries å’Œ skip_ssl çš„è«‹æ±‚ç‰©ä»¶
        background_tasks: FastAPI èƒŒæ™¯ä»»å‹™ç®¡ç†å™¨
        
    Returns:
        ä»»å‹™æ¥æ”¶ç¢ºèª
    """
    # åŠ å…¥èƒŒæ™¯ä»»å‹™
    background_tasks.add_task(
        process_and_webhook,
        request.url,
        request.webhook_url,
        request.metadata,
        request.max_retries,
        request.skip_ssl
    )
    
    return {
        "success": True,
        "message": "è§£æä»»å‹™å·²æ¥æ”¶ï¼Œå°‡åœ¨å®Œæˆå¾Œå›èª¿ webhook",
        "url": request.url,
        "webhook_url": request.webhook_url,
        "max_retries": request.max_retries
    }


@app.post("/api/decode-google-url")
async def decode_google_url_post(request: DecodeGoogleUrlRequest):
    """
    POST æ–¹æ³•ï¼šå¾ Google é‡å®šå‘ URL ä¸­æå–çœŸå¯¦çš„ç›®æ¨™ URL
    
    æ”¯æ´çš„æ ¼å¼ï¼š
    - Google News/Alerts: https://www.google.com/url?url=...
    - Google RSS: https://news.google.com/rss/articles/...
    
    Args:
        request: åŒ…å« url çš„è«‹æ±‚ç‰©ä»¶
        
    Returns:
        åŒ…å«åŸå§‹ URL å’Œè§£ç¢¼å¾Œ URL çš„ JSON å›æ‡‰
        
    Example:
        POST /api/decode-google-url
        {
            "url": "https://www.google.com/url?url=https://example.com/article&..."
        }
        
        Response:
        {
            "success": true,
            "original_url": "https://www.google.com/url?url=...",
            "decoded_url": "https://example.com/article",
            "is_google_url": true
        }
    """
    try:
        original_url = request.url
        decoded_url = decode_google_url(original_url)
        
        # æª¢æŸ¥æ˜¯å¦ç‚º Google URL
        is_google_url = 'google.com' in original_url
        
        return {
            "success": True,
            "original_url": original_url,
            "decoded_url": decoded_url,
            "is_google_url": is_google_url,
            "changed": original_url != decoded_url
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è§£ç¢¼ URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


@app.get("/api/decode-google-url")
async def decode_google_url_get(url: str):
    """
    GET æ–¹æ³•ï¼šå¾ Google é‡å®šå‘ URL ä¸­æå–çœŸå¯¦çš„ç›®æ¨™ URL
    
    Args:
        url: Google é‡å®šå‘ URLï¼ˆä½œç‚ºæŸ¥è©¢åƒæ•¸ï¼‰
        
    Returns:
        åŒ…å«åŸå§‹ URL å’Œè§£ç¢¼å¾Œ URL çš„ JSON å›æ‡‰
        
    Example:
        GET /api/decode-google-url?url=https://www.google.com/url?url=https://example.com/article
        
        Response:
        {
            "success": true,
            "original_url": "https://www.google.com/url?url=...",
            "decoded_url": "https://example.com/article",
            "is_google_url": true
        }
    """
    if not url:
        raise HTTPException(
            status_code=400,
            detail="è«‹æä¾› URL åƒæ•¸"
        )
    
    try:
        decoded_url = decode_google_url(url)
        
        # æª¢æŸ¥æ˜¯å¦ç‚º Google URL
        is_google_url = 'google.com' in url
        
        return {
            "success": True,
            "original_url": url,
            "decoded_url": decoded_url,
            "is_google_url": is_google_url,
            "changed": url != decoded_url
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è§£ç¢¼ URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


@app.get("/health")
@app.head("/health")  # æ”¯æŒ HEAD è«‹æ±‚
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "parser-api",
        "version": "1.7.0",
        "features": [
            "retry-mechanism",
            "enhanced-headers",
            "ssl-handling",
            "exponential-backoff",
            "playwright-dynamic-rendering",
            "ad-blocking",
            "anti-bot-detection",
            "lazy-loading-support",
            "concurrency-control",
            "container-optimized"
        ]
    }


if __name__ == "__main__":
    # å¾ç’°å¢ƒè®Šæ•¸è®€å–åŸ è™Ÿï¼ˆRailway æœƒæä¾›ï¼‰ï¼Œé è¨­ 3000
    port = int(os.getenv("PORT", 3000))
    
    print("ğŸš€ Parser ä¼ºæœå™¨å·²å•Ÿå‹•ï¼ï¼ˆPython å¢å¼·ç‰ˆ v1.7.0ï¼‰")
    print(f"ğŸ“¡ ç›£è½åŸ è™Ÿ: {port}")
    print(f"ğŸŒ æœ¬åœ°è¨ªå•: http://localhost:{port}")
    print(f"ğŸ“š API æ–‡ä»¶: http://localhost:{port}/docs")
    print("\nğŸ›¡ï¸  å¢å¼·åŠŸèƒ½:")
    print("  âœ“ è‡ªå‹•é‡è©¦æ©Ÿåˆ¶ï¼ˆ403/429 éŒ¯èª¤ï¼‰")
    print("  âœ“ éš¨æ©Ÿ User-Agent")
    print("  âœ“ SSL éŒ¯èª¤è™•ç†")
    print("  âœ“ æŒ‡æ•¸é€€é¿é‡è©¦")
    print("  âœ“ ä½µç™¼æ§åˆ¶ï¼ˆæœ€å¤š 2 å€‹åŒæ™‚é‹è¡Œçš„ç€è¦½å™¨ï¼‰")
    print("  âœ“ å®¹å™¨å„ªåŒ–ï¼ˆä¿®å¾© BlockingIOErrorï¼‰")
    print("\nä½¿ç”¨ç¯„ä¾‹:")
    print(f"  POST http://localhost:{port}/api/parse")
    print('  Body: {"url": "https://example.com/article", "max_retries": 3}')
    print("\n  æˆ–ä½¿ç”¨ GET:")
    print(f"  http://localhost:{port}/api/parse?url=https://example.com/article")
    print("\næŒ‰ Ctrl+C åœæ­¢ä¼ºæœå™¨\n")
    
    # å•Ÿå‹•ä¼ºæœå™¨
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )
