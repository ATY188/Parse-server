#!/usr/bin/env python3
"""
æ¸¬è©¦ Crawl4AI å°å›°é›£ç¶²ç«™çš„çˆ¬å–èƒ½åŠ›
å°æ¯”ä¹‹å‰å¤±æ•—çš„ç¶²ç«™
"""
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
import time

# æ¸¬è©¦ç¶²ç«™åˆ—è¡¨ï¼ˆä¹‹å‰å¤±æ•—çš„ï¼‰
FAILED_WEBSITES = [
    {
        "name": "Japan Times (Cloudflare)",
        "url": "https://www.japantimes.co.jp/news/2025/11/14/world/ukraine-uk-spy-chief/",
        "expected": "Intel shows Putin not ready"
    },
    {
        "name": "Reuters (401 Forbidden)",
        "url": "https://www.reuters.com/business/wall-st-week-ahead-skittish-tech-stock-investors-turn-nvidia-results-next-cues-2025-11-14/",
        "expected": "nvidia"
    },
    {
        "name": "KOIN News (403 Forbidden)",
        "url": "https://www.koin.com/news/oregon/intel-laying-off-hundreds-of-employees-in-oregon/",
        "expected": "Intel laying off"
    }
]

# æˆåŠŸç¶²ç«™ï¼ˆä½œç‚ºå°ç…§çµ„ï¼‰
SUCCESS_WEBSITES = [
    {
        "name": "é¢¨å‚³åª’ (æˆåŠŸé)",
        "url": "https://www.storm.mg/article/11081146",
        "expected": "è¼é”"
    },
    {
        "name": "éå‡¡æ–°è (æˆåŠŸé)",
        "url": "https://news.ustv.com.tw/newsdetail/20251113A001001",
        "expected": "å°ç©é›»"
    }
]

async def test_crawl4ai(url, name, expected_keyword):
    """ä½¿ç”¨ Crawl4AI æ¸¬è©¦å–®å€‹ç¶²ç«™"""
    print(f"\n{'='*80}")
    print(f"ğŸ§ª æ¸¬è©¦: {name}")
    print(f"ğŸ”— URL: {url[:60]}...")
    print(f"{'='*80}")
    
    start_time = time.time()
    
    try:
        # é…ç½®ç€è¦½å™¨ï¼ˆä½¿ç”¨ patchright ååµæ¸¬æ¨¡å¼ï¼‰
        browser_config = BrowserConfig(
            headless=True,
            verbose=False,
            extra_args=["--disable-blink-features=AutomationControlled"]
        )
        
        # é…ç½®çˆ¬å–å™¨ï¼ˆå•Ÿç”¨ magic æ¨¡å¼è‡ªå‹•è™•ç†åçˆ¬ï¼‰
        crawler_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            wait_until="networkidle",
            magic=True,  # ğŸ”¥ å•Ÿç”¨è‡ªå‹•åçˆ¬ç¹é
            page_timeout=30000,  # 30 ç§’è¶…æ™‚
        )
        
        async with AsyncWebCrawler(config=browser_config) as crawler:
            result = await crawler.arun(
                url=url,
                config=crawler_config
            )
            
            elapsed = time.time() - start_time
            
            # æª¢æŸ¥çµæœ
            if result.success:
                content_length = len(result.markdown or "")
                title = result.metadata.get('title', 'N/A')
                
                # æª¢æŸ¥æ˜¯å¦åŒ…å«é æœŸé—œéµå­—
                has_keyword = expected_keyword.lower() in (result.markdown or "").lower()
                
                print(f"âœ… æˆåŠŸè§£æ")
                print(f"â±ï¸  æ™‚é–“: {elapsed:.2f} ç§’")
                print(f"ğŸ“° æ¨™é¡Œ: {title[:80]}")
                print(f"ğŸ“ å…§å®¹é•·åº¦: {content_length} å­—å…ƒ")
                print(f"ğŸ” é—œéµå­— '{expected_keyword}': {'âœ… æ‰¾åˆ°' if has_keyword else 'âŒ æœªæ‰¾åˆ°'}")
                
                # é¡¯ç¤ºå‰ 200 å­—
                preview = (result.markdown or "")[:200].replace('\n', ' ')
                print(f"ğŸ“„ é è¦½: {preview}...")
                
                return {
                    "success": True,
                    "name": name,
                    "url": url,
                    "time": elapsed,
                    "content_length": content_length,
                    "has_keyword": has_keyword,
                    "title": title
                }
            else:
                print(f"âŒ è§£æå¤±æ•—")
                print(f"â±ï¸  æ™‚é–“: {elapsed:.2f} ç§’")
                print(f"â— éŒ¯èª¤: {result.error_message or 'Unknown error'}")
                
                return {
                    "success": False,
                    "name": name,
                    "url": url,
                    "time": elapsed,
                    "error": result.error_message
                }
                
    except Exception as e:
        elapsed = time.time() - start_time
        print(f"âŒ ç•°å¸¸éŒ¯èª¤")
        print(f"â±ï¸  æ™‚é–“: {elapsed:.2f} ç§’")
        print(f"ğŸ’¥ éŒ¯èª¤: {str(e)}")
        
        return {
            "success": False,
            "name": name,
            "url": url,
            "time": elapsed,
            "error": str(e)
        }

async def main():
    """ä¸»æ¸¬è©¦æµç¨‹"""
    print("\n" + "="*80)
    print("ğŸš€ Crawl4AI èƒ½åŠ›æ¸¬è©¦")
    print("="*80)
    
    results = []
    
    # æ¸¬è©¦å¤±æ•—ç¶²ç«™
    print("\nğŸ“‹ ç¬¬ä¸€çµ„ï¼šä¹‹å‰å¤±æ•—çš„ç¶²ç«™ï¼ˆCloudflareã€403ã€401ï¼‰")
    for site in FAILED_WEBSITES:
        result = await test_crawl4ai(site["url"], site["name"], site["expected"])
        results.append(result)
        await asyncio.sleep(2)  # é¿å…è«‹æ±‚å¤ªå¿«
    
    # æ¸¬è©¦æˆåŠŸç¶²ç«™ï¼ˆå°ç…§çµ„ï¼‰
    print("\nğŸ“‹ ç¬¬äºŒçµ„ï¼šä¹‹å‰æˆåŠŸçš„ç¶²ç«™ï¼ˆé©—è­‰åŸºæœ¬åŠŸèƒ½ï¼‰")
    for site in SUCCESS_WEBSITES:
        result = await test_crawl4ai(site["url"], site["name"], site["expected"])
        results.append(result)
        await asyncio.sleep(2)
    
    # çµ±è¨ˆçµæœ
    print("\n" + "="*80)
    print("ğŸ“Š æ¸¬è©¦çµæœçµ±è¨ˆ")
    print("="*80)
    
    total = len(results)
    success_count = sum(1 for r in results if r["success"])
    failed_count = total - success_count
    
    print(f"\nç¸½æ¸¬è©¦æ•¸: {total}")
    print(f"âœ… æˆåŠŸ: {success_count} ({success_count/total*100:.1f}%)")
    print(f"âŒ å¤±æ•—: {failed_count} ({failed_count/total*100:.1f}%)")
    
    print("\nè©³ç´°çµæœ:")
    for i, r in enumerate(results, 1):
        status = "âœ…" if r["success"] else "âŒ"
        print(f"{i}. {status} {r['name']}")
        print(f"   æ™‚é–“: {r['time']:.2f}s")
        if r["success"]:
            print(f"   å…§å®¹: {r['content_length']} å­—å…ƒ")
            print(f"   é—œéµå­—: {'âœ…' if r.get('has_keyword') else 'âŒ'}")
        else:
            print(f"   éŒ¯èª¤: {r.get('error', 'Unknown')[:80]}")
    
    print("\n" + "="*80)
    print("ğŸ¯ çµè«–")
    print("="*80)
    
    # åˆ†æå¤±æ•—ç¶²ç«™çš„è¡¨ç¾
    failed_sites_results = results[:len(FAILED_WEBSITES)]
    failed_sites_success = sum(1 for r in failed_sites_results if r["success"])
    
    print(f"\nğŸ”¥ ä¹‹å‰å¤±æ•—çš„ç¶²ç«™:")
    print(f"   æˆåŠŸçªç ´: {failed_sites_success}/{len(FAILED_WEBSITES)} å€‹")
    print(f"   æˆåŠŸç‡: {failed_sites_success/len(FAILED_WEBSITES)*100:.1f}%")
    
    if failed_sites_success > 0:
        print("\nâœ¨ Crawl4AI å±•ç¾äº†çªç ´åçˆ¬æ©Ÿåˆ¶çš„èƒ½åŠ›ï¼")
    else:
        print("\nâš ï¸ Crawl4AI ä»ç„¶ç„¡æ³•çªç ´é€™äº›ç¶²ç«™çš„é˜²è­·")
    
    # è¼¸å‡ºå»ºè­°
    print("\nğŸ’¡ å»ºè­°:")
    if failed_sites_success >= 2:
        print("   âœ… å€¼å¾—æ•´åˆåˆ°ä½ çš„ Parser API")
        print("   âœ… å¯ä»¥è™•ç† Cloudflare ç­‰åçˆ¬æ©Ÿåˆ¶")
    elif failed_sites_success == 1:
        print("   âš ï¸ æœ‰æ½›åŠ›ä½†æ•ˆæœæœ‰é™")
        print("   âš ï¸ å»ºè­°é‡å°ç‰¹å®šç¶²ç«™ä½¿ç”¨")
    else:
        print("   âŒ ä¸å»ºè­°å®Œå…¨æ›¿æ›ç¾æœ‰æ–¹æ¡ˆ")
        print("   âœ… å»ºè­°ä¿æŒ RSS é™ç´šç­–ç•¥")

if __name__ == "__main__":
    asyncio.run(main())

