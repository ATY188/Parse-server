"""
ç¶²é å…§å®¹è§£æå™¨ API - Python ç‰ˆæœ¬ï¼ˆå¢å¼·ç‰ˆï¼‰
ä½¿ç”¨ FastAPI + trafilatura
æ”¯æ´é‡è©¦ã€æ›´å¥½çš„ headersã€SSL éŒ¯èª¤è™•ç†

å®‰è£å¥—ä»¶ï¼š
pip install fastapi uvicorn trafilatura httpx python-multipart

å•Ÿå‹•æ–¹å¼ï¼š
python parser-server.py
æˆ–
uvicorn parser-server:app --reload --port 3000
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from pydantic import BaseModel, HttpUrl, validator
from typing import Optional, Dict, Any
import trafilatura
import httpx
from datetime import datetime
import uvicorn
import os
import asyncio
import random

# å»ºç«‹ FastAPI æ‡‰ç”¨
app = FastAPI(
    title="ç¶²é å…§å®¹è§£æå™¨ APIï¼ˆå¢å¼·ç‰ˆï¼‰",
    description="ä½¿ç”¨ trafilatura è‡ªå‹•æå–ç¶²é æ–‡ç« å…§å®¹ï¼Œæ”¯æ´é‡è©¦å’ŒéŒ¯èª¤è™•ç†",
    version="1.2.0"
)

# å¤šçµ„ User-Agent è¼ªæµä½¿ç”¨
USER_AGENTS = [
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

def get_random_user_agent():
    """éš¨æ©Ÿé¸æ“‡ User-Agent"""
    return random.choice(USER_AGENTS)

def get_enhanced_headers(url: str):
    """ç²å–å¢å¼·çš„ HTTP headers"""
    from urllib.parse import urlparse
    parsed_url = urlparse(url)
    
    return {
        'User-Agent': get_random_user_agent(),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Referer': f'https://{parsed_url.netloc}/',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0'
    }

# è«‹æ±‚è³‡æ–™æ¨¡å‹
class ParseRequest(BaseModel):
    url: str
    max_retries: Optional[int] = 3
    skip_ssl: Optional[bool] = False
    
    @validator('url')
    def validate_url(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­')
        return v

class ParseWebhookRequest(BaseModel):
    url: str
    webhook_url: str
    metadata: Optional[Dict[str, Any]] = {}
    max_retries: Optional[int] = 3
    skip_ssl: Optional[bool] = False
    
    @validator('url', 'webhook_url')
    def validate_urls(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­')
        return v


# é¦–é è·¯ç”±
@app.get("/")
async def root():
    """API é¦–é  - é¡¯ç¤ºå¯ç”¨ç«¯é»"""
    return {
        "message": "æ­¡è¿ä½¿ç”¨ç¶²é å…§å®¹è§£æå™¨ API (Python å¢å¼·ç‰ˆ)",
        "framework": "FastAPI + trafilatura",
        "version": "1.2.0",
        "features": [
            "è‡ªå‹•é‡è©¦æ©Ÿåˆ¶ï¼ˆè™•ç† 403/429 éŒ¯èª¤ï¼‰",
            "éš¨æ©Ÿ User-Agent",
            "å¢å¼·çš„ HTTP headers",
            "SSL éŒ¯èª¤è™•ç†",
            "æŒ‡æ•¸é€€é¿ï¼ˆExponential Backoffï¼‰"
        ],
        "endpoints": {
            "parse": {
                "method": "POST",
                "path": "/api/parse",
                "body": {
                    "url": "è¦è§£æçš„ç¶²é  URL",
                    "max_retries": "(é¸å¡«) æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œé è¨­ 3",
                    "skip_ssl": "(é¸å¡«) è·³é SSL é©—è­‰ï¼Œé è¨­ false"
                },
                "description": "è§£ææŒ‡å®š URL çš„ç¶²é å…§å®¹ï¼ˆåŒæ­¥å›å‚³ï¼Œæ”¯æ´é‡è©¦ï¼‰"
            },
            "parseGet": {
                "method": "GET",
                "path": "/api/parse?url=YOUR_URL",
                "description": "ä½¿ç”¨ GET æ–¹æ³•è§£æç¶²é å…§å®¹"
            },
            "parseWebhook": {
                "method": "POST",
                "path": "/api/parse-webhook",
                "body": {
                    "url": "è¦è§£æçš„ç¶²é  URL",
                    "webhook_url": "n8n webhook URL",
                    "metadata": "(é¸å¡«) é¡å¤–è³‡æ–™",
                    "max_retries": "(é¸å¡«) æœ€å¤§é‡è©¦æ¬¡æ•¸",
                    "skip_ssl": "(é¸å¡«) è·³é SSL é©—è­‰"
                },
                "description": "è§£æç¶²é ä¸¦å›èª¿ webhookï¼ˆé©ç”¨æ–¼ n8n æ•´åˆï¼‰"
            },
            "docs": {
                "method": "GET",
                "path": "/docs",
                "description": "Swagger UI äº’å‹•å¼ API æ–‡ä»¶"
            }
        },
        "examples": [
            'POST /api/parse with body: {"url": "https://example.com/article", "max_retries": 3}',
            'GET /api/parse?url=https://example.com/article',
            'POST /api/parse-webhook with body: {"url": "https://example.com/article", "webhook_url": "https://your-n8n.com/webhook/..."}'
        ],
        "errorHandling": {
            "403 Forbidden": "è‡ªå‹•é‡è©¦ + éš¨æ©Ÿ User-Agent + Referer header",
            "429 Too Many Requests": "æŒ‡æ•¸é€€é¿é‡è©¦ï¼ˆ2s, 4s, 8s...ï¼‰",
            "SSL Certificate Error": "å¯é¸æ“‡è·³é SSL é©—è­‰ï¼ˆskip_ssl: trueï¼‰"
        },
        "documentation": "è¨ªå• /docs æŸ¥çœ‹å®Œæ•´ API æ–‡ä»¶"
    }


async def fetch_and_parse_with_retry(
    url: str, 
    max_retries: int = 3, 
    skip_ssl: bool = False
) -> Dict[str, Any]:
    """
    ä¸‹è¼‰ä¸¦è§£æç¶²é å…§å®¹ï¼ˆæ”¯æ´é‡è©¦ï¼‰
    
    Args:
        url: è¦è§£æçš„ç¶²é  URL
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        skip_ssl: æ˜¯å¦è·³é SSL é©—è­‰
        
    Returns:
        è§£æå¾Œçš„è³‡æ–™å­—å…¸
        
    Raises:
        HTTPException: ç•¶ä¸‹è¼‰æˆ–è§£æå¤±æ•—æ™‚
    """
    last_error = None
    
    for attempt in range(1, max_retries + 1):
        try:
            print(f"[å˜—è©¦ {attempt}/{max_retries}] è§£æ: {url}")
            
            # ç²å–å¢å¼·çš„ headers
            headers = get_enhanced_headers(url)
            
            # è¨­å®š timeout å’Œ SSL é©—è­‰
            timeout = httpx.Timeout(30.0, connect=10.0)
            verify_ssl = not skip_ssl
            
            # ä¸‹è¼‰ç¶²é å…§å®¹
            async with httpx.AsyncClient(
                timeout=timeout,
                verify=verify_ssl,
                follow_redirects=True,
                headers=headers
            ) as client:
                response = await client.get(url)
                response.raise_for_status()
                html_content = response.text
            
            # ä½¿ç”¨ trafilatura è§£æå…§å®¹
            text_content = trafilatura.extract(
                html_content,
                include_comments=False,
                include_tables=True,
                no_fallback=False
            )
            
            # æå–å®Œæ•´è³‡è¨Šï¼ˆåŒ…å«å…ƒæ•¸æ“šï¼‰
            metadata = trafilatura.extract_metadata(html_content)
            
            # æå– HTML æ ¼å¼çš„å…§å®¹
            html_formatted = trafilatura.extract(
                html_content,
                include_comments=False,
                include_tables=True,
                no_fallback=False,
                output_format='xml'
            )
            
            # æ•´ç†å›å‚³è³‡æ–™
            parsed_data = {
                "title": metadata.title if metadata else None,
                "author": metadata.author if metadata else None,
                "date_published": metadata.date if metadata else None,
                "url": metadata.url if metadata else url,
                "domain": metadata.sitename if metadata else None,
                "description": metadata.description if metadata else None,
                "categories": metadata.categories if metadata else None,
                "tags": metadata.tags if metadata else None,
                "content": html_formatted or text_content,
                "text_content": text_content,
                "excerpt": text_content[:200] + "..." if text_content and len(text_content) > 200 else text_content,
                "word_count": len(text_content.split()) if text_content else 0,
                "language": metadata.language if metadata else None
            }
            
            title_preview = parsed_data.get('title') or 'No title'
            print(f"[æˆåŠŸ] å˜—è©¦ {attempt}: {title_preview[:50] if title_preview else 'No title'}")
            return {
                "success": True,
                "data": parsed_data,
                "attempt": attempt,
                "retries": attempt - 1
            }
            
        except httpx.HTTPStatusError as e:
            last_error = e
            status_code = e.response.status_code
            print(f"[å¤±æ•—] å˜—è©¦ {attempt}: HTTP {status_code} - {str(e)}")
            
            # å¦‚æœæ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œæ‹‹å‡ºéŒ¯èª¤
            if attempt == max_retries:
                raise HTTPException(
                    status_code=500,
                    detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: Client error '{status_code} {e.response.reason_phrase}' for url '{url}'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/{status_code}"
                )
            
            # æ ¹æ“šéŒ¯èª¤é¡å‹æ±ºå®šç­‰å¾…æ™‚é–“
            if status_code in [429, 403]:
                # 429 Too Many Requests æˆ– 403 Forbiddenï¼šæŒ‡æ•¸é€€é¿
                wait_time = (2 ** attempt)  # 2ç§’ã€4ç§’ã€8ç§’...
                print(f"[ç­‰å¾…] {wait_time} ç§’å¾Œé‡è©¦ï¼ˆHTTP {status_code}ï¼‰...")
                await asyncio.sleep(wait_time)
            else:
                # å…¶ä»–éŒ¯èª¤ï¼šçŸ­æš«ç­‰å¾…
                await asyncio.sleep(1)
                
        except httpx.ConnectError as e:
            last_error = e
            print(f"[å¤±æ•—] å˜—è©¦ {attempt}: é€£æ¥éŒ¯èª¤ - {str(e)}")
            
            if attempt == max_retries:
                raise HTTPException(
                    status_code=500,
                    detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: ç„¡æ³•é€£æ¥åˆ° {url}"
                )
            
            await asyncio.sleep(2)
            
        except Exception as e:
            error_msg = str(e)
            print(f"[å¤±æ•—] å˜—è©¦ {attempt}: {error_msg}")
            
            # SSL éŒ¯èª¤è™•ç†
            if "SSL" in error_msg or "certificate" in error_msg.lower():
                last_error = e
                
                if attempt == max_retries:
                    if skip_ssl:
                        raise HTTPException(
                            status_code=500,
                            detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: {error_msg}"
                        )
                    else:
                        raise HTTPException(
                            status_code=500,
                            detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: {error_msg}\n\nğŸ’¡ æç¤ºï¼šå¯ä»¥å˜—è©¦è¨­å®š skip_ssl: true ä¾†è·³é SSL é©—è­‰"
                        )
                
                # ä¸‹æ¬¡å˜—è©¦æ™‚è·³é SSL é©—è­‰
                if not skip_ssl:
                    print(f"[SSL éŒ¯èª¤] ä¸‹æ¬¡å°‡è·³é SSL é©—è­‰...")
                    skip_ssl = True
                    
                await asyncio.sleep(1)
            else:
                # å…¶ä»–éŒ¯èª¤
                last_error = e
                
                if attempt == max_retries:
                    raise HTTPException(
                        status_code=500,
                        detail=f"è§£æç¶²é å¤±æ•—: {error_msg}"
                    )
                
                await asyncio.sleep(1)
    
    # ç†è«–ä¸Šä¸æœƒåˆ°é”é€™è£¡ï¼Œä½†ä»¥é˜²è¬ä¸€
    raise HTTPException(
        status_code=500,
        detail=f"è§£æç¶²é å¤±æ•—: {str(last_error)}"
    )


@app.post("/api/parse")
async def parse_url(request: ParseRequest):
    """
    POST æ–¹æ³•ï¼šè§£æç¶²é å…§å®¹ï¼ˆæ”¯æ´é‡è©¦ï¼‰
    
    Args:
        request: åŒ…å« urlã€max_retries å’Œ skip_ssl çš„è«‹æ±‚ç‰©ä»¶
        
    Returns:
        è§£æå¾Œçš„ç¶²é å…§å®¹
    """
    print(f"æ­£åœ¨è§£æ: {request.url} (max_retries: {request.max_retries}, skip_ssl: {request.skip_ssl})")
    
    try:
        result = await fetch_and_parse_with_retry(
            request.url,
            max_retries=request.max_retries,
            skip_ssl=request.skip_ssl
        )
        
        return result
        
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"è§£æéŒ¯èª¤: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"è§£æç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


@app.get("/api/parse")
async def parse_url_get(url: str, max_retries: int = 3, skip_ssl: bool = False):
    """
    GET æ–¹æ³•ï¼šè§£æç¶²é å…§å®¹ï¼ˆé€é query stringï¼‰
    
    Args:
        url: è¦è§£æçš„ç¶²é  URL
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ˆé è¨­ 3ï¼‰
        skip_ssl: æ˜¯å¦è·³é SSL é©—è­‰ï¼ˆé è¨­ Falseï¼‰
        
    Returns:
        è§£æå¾Œçš„ç¶²é å…§å®¹
    """
    if not url:
        raise HTTPException(
            status_code=400,
            detail="è«‹åœ¨ URL åƒæ•¸ä¸­æä¾›è¦è§£æçš„ç¶²å€"
        )
    
    print(f"æ­£åœ¨è§£æ (GET): {url}")
    
    try:
        result = await fetch_and_parse_with_retry(
            url,
            max_retries=max_retries,
            skip_ssl=skip_ssl
        )
        
        return result
        
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"è§£æéŒ¯èª¤: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"è§£æç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


async def process_and_webhook(
    url: str, 
    webhook_url: str, 
    metadata: Dict[str, Any],
    max_retries: int = 3,
    skip_ssl: bool = False
):
    """
    èƒŒæ™¯ä»»å‹™ï¼šè§£æç¶²é ä¸¦å›èª¿ webhook
    
    Args:
        url: è¦è§£æçš„ç¶²é  URL
        webhook_url: webhook å›èª¿ URL
        metadata: é¡å¤–çš„å…ƒæ•¸æ“š
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        skip_ssl: æ˜¯å¦è·³é SSL é©—è­‰
    """
    print(f"æ­£åœ¨è§£æ (webhook æ¨¡å¼): {url}")
    
    try:
        # è§£æç¶²é ï¼ˆä½¿ç”¨é‡è©¦æ©Ÿåˆ¶ï¼‰
        result = await fetch_and_parse_with_retry(url, max_retries, skip_ssl)
        
        # æº–å‚™å›èª¿è³‡æ–™
        webhook_data = {
            "success": True,
            "original_url": url,
            "metadata": metadata,
            "parsed_data": result.get("data"),
            "attempt": result.get("attempt"),
            "retries": result.get("retries"),
            "parsed_at": datetime.now().isoformat()
        }
        
        # å›èª¿ webhook
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                webhook_url,
                json=webhook_data
            )
            
            if response.status_code == 200:
                print(f"âœ… Webhook å›èª¿æˆåŠŸ: {webhook_url}")
            else:
                print(f"âŒ Webhook å›èª¿å¤±æ•— ({response.status_code}): {webhook_url}")
                
    except Exception as e:
        print(f"è§£ææˆ–å›èª¿éŒ¯èª¤: {str(e)}")
        
        # å˜—è©¦å›èª¿éŒ¯èª¤è¨Šæ¯
        try:
            error_data = {
                "success": False,
                "original_url": url,
                "metadata": metadata,
                "error": str(e),
                "failed_at": datetime.now().isoformat()
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                await client.post(webhook_url, json=error_data)
                
        except Exception as webhook_error:
            print(f"ç„¡æ³•å›èª¿éŒ¯èª¤è¨Šæ¯: {str(webhook_error)}")


@app.post("/api/parse-webhook")
async def parse_url_webhook(request: ParseWebhookRequest, background_tasks: BackgroundTasks):
    """
    POST æ–¹æ³•ï¼šè§£æç¶²é ä¸¦å›èª¿ webhookï¼ˆç”¨æ–¼ n8n æ•´åˆï¼‰
    
    Args:
        request: åŒ…å« urlã€webhook_urlã€metadataã€max_retries å’Œ skip_ssl çš„è«‹æ±‚ç‰©ä»¶
        background_tasks: FastAPI èƒŒæ™¯ä»»å‹™ç®¡ç†å™¨
        
    Returns:
        ä»»å‹™æ¥æ”¶ç¢ºèª
    """
    # åŠ å…¥èƒŒæ™¯ä»»å‹™
    background_tasks.add_task(
        process_and_webhook,
        request.url,
        request.webhook_url,
        request.metadata,
        request.max_retries,
        request.skip_ssl
    )
    
    return {
        "success": True,
        "message": "è§£æä»»å‹™å·²æ¥æ”¶ï¼Œå°‡åœ¨å®Œæˆå¾Œå›èª¿ webhook",
        "url": request.url,
        "webhook_url": request.webhook_url,
        "max_retries": request.max_retries
    }


@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "parser-api",
        "version": "1.2.0",
        "features": [
            "retry-mechanism",
            "enhanced-headers",
            "ssl-handling",
            "exponential-backoff"
        ]
    }


if __name__ == "__main__":
    # å¾ç’°å¢ƒè®Šæ•¸è®€å–åŸ è™Ÿï¼ˆRailway æœƒæä¾›ï¼‰ï¼Œé è¨­ 3000
    port = int(os.getenv("PORT", 3000))
    
    print("ğŸš€ Parser ä¼ºæœå™¨å·²å•Ÿå‹•ï¼ï¼ˆPython å¢å¼·ç‰ˆ v1.2.0ï¼‰")
    print(f"ğŸ“¡ ç›£è½åŸ è™Ÿ: {port}")
    print(f"ğŸŒ æœ¬åœ°è¨ªå•: http://localhost:{port}")
    print(f"ğŸ“š API æ–‡ä»¶: http://localhost:{port}/docs")
    print("\nğŸ›¡ï¸  å¢å¼·åŠŸèƒ½:")
    print("  âœ“ è‡ªå‹•é‡è©¦æ©Ÿåˆ¶ï¼ˆ403/429 éŒ¯èª¤ï¼‰")
    print("  âœ“ éš¨æ©Ÿ User-Agent")
    print("  âœ“ SSL éŒ¯èª¤è™•ç†")
    print("  âœ“ æŒ‡æ•¸é€€é¿é‡è©¦")
    print("\nä½¿ç”¨ç¯„ä¾‹:")
    print(f"  POST http://localhost:{port}/api/parse")
    print('  Body: {"url": "https://example.com/article", "max_retries": 3}')
    print("\n  æˆ–ä½¿ç”¨ GET:")
    print(f"  http://localhost:{port}/api/parse?url=https://example.com/article")
    print("\næŒ‰ Ctrl+C åœæ­¢ä¼ºæœå™¨\n")
    
    # å•Ÿå‹•ä¼ºæœå™¨
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )
