"""
ç¶²é å…§å®¹è§£æå™¨ API - Python ç‰ˆæœ¬ï¼ˆå¢å¼·ç‰ˆï¼‰
ä½¿ç”¨ FastAPI + trafilatura
æ”¯æ´é‡è©¦ã€æ›´å¥½çš„ headersã€SSL éŒ¯èª¤è™•ç†

å®‰è£å¥—ä»¶ï¼š
pip install fastapi uvicorn trafilatura httpx python-multipart

å•Ÿå‹•æ–¹å¼ï¼š
python parser-server.py
æˆ–
uvicorn parser-server:app --reload --port 3000
"""

from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.responses import JSONResponse
from pydantic import BaseModel, HttpUrl, validator
from typing import Optional, Dict, Any
import trafilatura
import httpx
from datetime import datetime
import uvicorn
import os
import asyncio
import random
from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeout

# å»ºç«‹ FastAPI æ‡‰ç”¨
app = FastAPI(
    title="ç¶²é å…§å®¹è§£æå™¨ APIï¼ˆå¢å¼·ç‰ˆï¼‰",
    description="ä½¿ç”¨ trafilatura è‡ªå‹•æå–ç¶²é æ–‡ç« å…§å®¹ï¼Œæ”¯æ´é‡è©¦å’ŒéŒ¯èª¤è™•ç†",
    version="1.2.0"
)

# å¤šçµ„ User-Agent è¼ªæµä½¿ç”¨
USER_AGENTS = [
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Safari/605.1.15',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:120.0) Gecko/20100101 Firefox/120.0',
    'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
]

def get_random_user_agent():
    """éš¨æ©Ÿé¸æ“‡ User-Agent"""
    return random.choice(USER_AGENTS)

def get_enhanced_headers(url: str):
    """ç²å–å¢å¼·çš„ HTTP headers"""
    from urllib.parse import urlparse
    parsed_url = urlparse(url)
    
    return {
        'User-Agent': get_random_user_agent(),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
        'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Referer': f'https://{parsed_url.netloc}/',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'none',
        'Cache-Control': 'max-age=0'
    }

# è«‹æ±‚è³‡æ–™æ¨¡å‹
class ParseRequest(BaseModel):
    url: str
    max_retries: Optional[int] = 3
    skip_ssl: Optional[bool] = False
    
    @validator('url')
    def validate_url(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­')
        return v

class ParseDynamicRequest(BaseModel):
    url: str
    wait_for: Optional[str] = None
    block_ads: Optional[bool] = True  # é è¨­å±è”½å»£å‘Š
    stealth_mode: Optional[bool] = True  # é è¨­å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼
    
    @validator('url')
    def validate_url(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­')
        return v

class ParseWebhookRequest(BaseModel):
    url: str
    webhook_url: str
    metadata: Optional[Dict[str, Any]] = {}
    max_retries: Optional[int] = 3
    skip_ssl: Optional[bool] = False
    
    @validator('url', 'webhook_url')
    def validate_urls(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­')
        return v

class DecodeGoogleUrlRequest(BaseModel):
    url: str
    
    @validator('url')
    def validate_url(cls, v):
        if not v.startswith(('http://', 'https://')):
            raise ValueError('URL å¿…é ˆä»¥ http:// æˆ– https:// é–‹é ­')
        return v


async def fetch_with_playwright(
    url: str, 
    wait_for: Optional[str] = None,
    block_ads: bool = True,
    stealth_mode: bool = True
) -> str:
    """
    ä½¿ç”¨ Playwright ç²å–å‹•æ…‹ç¶²é å…§å®¹ï¼ˆå¢å¼·ç‰ˆï¼‰
    
    Args:
        url: è¦è¨ªå•çš„ç¶²é  URL
        wait_for: ç­‰å¾…ç‰¹å®šå…ƒç´ ï¼ˆCSS selectorï¼‰å‡ºç¾ï¼Œä¾‹å¦‚ 'article' æˆ– '.content'
        block_ads: æ˜¯å¦å±è”½å»£å‘Šï¼ˆé è¨­ Trueï¼‰
        stealth_mode: æ˜¯å¦å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼ï¼ˆé è¨­ Trueï¼‰
        
    Returns:
        æ¸²æŸ“å¾Œçš„ HTML å…§å®¹
        
    Raises:
        Exception: ç•¶ç€è¦½å™¨æ“ä½œå¤±æ•—æ™‚
    """
    async with async_playwright() as p:
        try:
            # å•Ÿå‹• Chromium ç€è¦½å™¨ï¼ˆç„¡é ­æ¨¡å¼ï¼‰
            print(f"[Playwright] å•Ÿå‹•ç€è¦½å™¨...")
            browser = await p.chromium.launch(
                headless=True,
                args=[
                    '--disable-blink-features=AutomationControlled',  # ç¦ç”¨è‡ªå‹•åŒ–æ§åˆ¶ç‰¹å¾µ
                    '--no-sandbox',
                    '--disable-setuid-sandbox',
                ]
            )
            
            # å‰µå»ºæ–°çš„ç€è¦½å™¨ä¸Šä¸‹æ–‡ï¼ˆæ¨¡æ“¬çœŸå¯¦ç”¨æˆ¶ï¼‰
            context = await browser.new_context(
                user_agent=get_random_user_agent(),
                viewport={'width': 1920, 'height': 1080},
                locale='zh-TW',
                timezone_id='Asia/Taipei',
                color_scheme='light',
                extra_http_headers={
                    'Accept-Language': 'zh-TW,zh;q=0.9,en-US;q=0.8,en;q=0.7',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                }
            )
            
            # å¦‚æœå•Ÿç”¨å»£å‘Šå±è”½
            if block_ads:
                print(f"[Playwright] å•Ÿç”¨å»£å‘Šå±è”½")
                ad_domains = [
                    'doubleclick.net', 'googlesyndication.com', 'googletagmanager.com',
                    'google-analytics.com', 'facebook.com/tr/', 'scorecardresearch.com',
                    'ad.doubleclick.net', 'static.ads-twitter.com', 'ads.yahoo.com',
                    'pagead2.googlesyndication.com', 'adservice.google.com',
                    'analytics.google.com', 'googleadservices.com'
                ]
                
                await context.route("**/*", lambda route: (
                    route.abort() if any(ad in route.request.url for ad in ad_domains)
                    else route.continue_()
                ))
            
            # å‰µå»ºæ–°é é¢
            page = await context.new_page()
            
            # å¦‚æœå•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼
            if stealth_mode:
                print(f"[Playwright] å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼")
                # éš±è— webdriver ç‰¹å¾µ
                await page.add_init_script("""
                    // ç§»é™¤ webdriver æ¨™è¨˜
                    Object.defineProperty(navigator, 'webdriver', {
                        get: () => false
                    });
                    
                    // å½è£ Chrome å°è±¡
                    window.chrome = {
                        runtime: {}
                    };
                    
                    // ä¿®æ”¹ permissions
                    const originalQuery = window.navigator.permissions.query;
                    window.navigator.permissions.query = (parameters) => (
                        parameters.name === 'notifications' ?
                            Promise.resolve({ state: Notification.permission }) :
                            originalQuery(parameters)
                    );
                    
                    // å½è£ plugins
                    Object.defineProperty(navigator, 'plugins', {
                        get: () => [1, 2, 3, 4, 5]
                    });
                    
                    // å½è£ languages
                    Object.defineProperty(navigator, 'languages', {
                        get: () => ['zh-TW', 'zh', 'en-US', 'en']
                    });
                """)
            
            # è¨ªå•ç¶²é 
            print(f"[Playwright] æ­£åœ¨è¨ªå•: {url}")
            await page.goto(url, wait_until='networkidle', timeout=60000)  # å¢åŠ åˆ° 60 ç§’
            
            # éš¨æ©Ÿå»¶é²ï¼ˆæ¨¡æ“¬äººé¡è¡Œç‚ºï¼‰
            delay = random.uniform(1, 2.5)
            print(f"[Playwright] éš¨æ©Ÿå»¶é² {delay:.1f} ç§’...")
            await asyncio.sleep(delay)
            
            # ç§»é™¤å»£å‘Šå…ƒç´ ï¼ˆDOM å±¤é¢ï¼‰
            if block_ads:
                await page.evaluate("""() => {
                    // ç§»é™¤å¸¸è¦‹å»£å‘Šå…ƒç´ 
                    const selectors = [
                        '[class*="ad-"]', '[class*="ad_"]', '[id*="ad-"]', '[id*="ad_"]',
                        '[class*="advertisement"]', '[class*="banner"]',
                        'iframe[src*="ads"]', 'iframe[src*="doubleclick"]',
                        '.ad', '.ads', '#ad', '#ads'
                    ];
                    
                    selectors.forEach(selector => {
                        try {
                            document.querySelectorAll(selector).forEach(el => el.remove());
                        } catch(e) {}
                    });
                }""")
            
            # å¦‚æœæŒ‡å®šäº†ç­‰å¾…å…ƒç´ ï¼Œç­‰å¾…è©²å…ƒç´ å‡ºç¾
            if wait_for:
                print(f"[Playwright] ç­‰å¾…å…ƒç´ : {wait_for}")
                try:
                    await page.wait_for_selector(wait_for, timeout=20000)  # å¢åŠ åˆ° 20 ç§’
                except:
                    print(f"[Playwright] è­¦å‘Šï¼šå…ƒç´  {wait_for} æœªæ‰¾åˆ°ï¼Œç¹¼çºŒæå–å…§å®¹")
            
            # æ»¾å‹•é é¢ä»¥è§¸ç™¼æ‡¶åŠ è¼‰
            print(f"[Playwright] æ»¾å‹•é é¢ä»¥è¼‰å…¥å‹•æ…‹å…§å®¹...")
            await page.evaluate("""async () => {
                await new Promise((resolve) => {
                    let totalHeight = 0;
                    const distance = 100;
                    const timer = setInterval(() => {
                        const scrollHeight = document.body.scrollHeight;
                        window.scrollBy(0, distance);
                        totalHeight += distance;
                        
                        if(totalHeight >= scrollHeight){
                            clearInterval(timer);
                            resolve();
                        }
                    }, 100);
                });
            }""")
            
            # å†ç­‰å¾…ä¸€ä¸‹ï¼Œç¢ºä¿å…§å®¹è¼‰å…¥å®Œæˆ
            await asyncio.sleep(1)
            
            # ç²å–æ¸²æŸ“å¾Œçš„ HTML
            html_content = await page.content()
            
            # é—œé–‰ç€è¦½å™¨
            await browser.close()
            
            print(f"[Playwright] âœ… æˆåŠŸç²å–å…§å®¹ï¼Œé•·åº¦: {len(html_content)}")
            return html_content
            
        except PlaywrightTimeout as e:
            raise Exception(f"Playwright è¶…æ™‚: {str(e)}")
        except Exception as e:
            raise Exception(f"Playwright éŒ¯èª¤: {str(e)}")


async def fetch_and_parse_with_playwright(
    url: str,
    wait_for: Optional[str] = None,
    block_ads: bool = True,
    stealth_mode: bool = True
) -> Dict[str, Any]:
    """
    ä½¿ç”¨ Playwright ä¸‹è¼‰ä¸¦è§£æå‹•æ…‹ç¶²é å…§å®¹ï¼ˆå¢å¼·ç‰ˆï¼‰
    
    Args:
        url: è¦è§£æçš„ç¶²é  URL
        wait_for: ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç¾
        block_ads: æ˜¯å¦å±è”½å»£å‘Š
        stealth_mode: æ˜¯å¦å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼
        
    Returns:
        è§£æå¾Œçš„è³‡æ–™å­—å…¸
    """
    try:
        # ä½¿ç”¨ Playwright ç²å–æ¸²æŸ“å¾Œçš„ HTML
        html_content = await fetch_with_playwright(url, wait_for, block_ads, stealth_mode)
        
        # ä½¿ç”¨ trafilatura è§£æå…§å®¹
        try:
            text_content = trafilatura.extract(
                html_content,
                include_comments=False,
                include_tables=True,
                no_fallback=False
            )
        except Exception as e:
            print(f"[è­¦å‘Š] trafilatura.extract å¤±æ•—: {e}")
            text_content = None
        
        # æå–å…ƒæ•¸æ“š
        try:
            metadata = trafilatura.extract_metadata(html_content)
        except Exception as e:
            print(f"[è­¦å‘Š] trafilatura.extract_metadata å¤±æ•—: {e}")
            metadata = None
        
        # æå– XML æ ¼å¼å…§å®¹
        try:
            html_formatted = trafilatura.extract(
                html_content,
                include_comments=False,
                include_tables=True,
                no_fallback=False,
                output_format='xml'
            )
        except Exception as e:
            print(f"[è­¦å‘Š] trafilatura.extract (XML) å¤±æ•—: {e}")
            html_formatted = None
        
        # æ•´ç†å›å‚³è³‡æ–™
        parsed_data = {
            "title": getattr(metadata, 'title', None) if metadata else None,
            "author": getattr(metadata, 'author', None) if metadata else None,
            "date_published": getattr(metadata, 'date', None) if metadata else None,
            "url": getattr(metadata, 'url', url) if metadata else url,
            "domain": getattr(metadata, 'sitename', None) if metadata else None,
            "description": getattr(metadata, 'description', None) if metadata else None,
            "categories": getattr(metadata, 'categories', None) if metadata else None,
            "tags": getattr(metadata, 'tags', None) if metadata else None,
            "content": html_formatted or text_content,
            "text_content": text_content,
            "excerpt": text_content[:200] + "..." if text_content and len(text_content) > 200 else text_content,
            "word_count": len(text_content.split()) if text_content else 0,
            "language": getattr(metadata, 'language', None) if metadata else None,
            "rendering_method": "playwright"
        }
        
        return {
            "success": True,
            "data": parsed_data,
            "method": "playwright"
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"ä½¿ç”¨ Playwright è§£æå¤±æ•—: {str(e)}"
        )


def decode_google_url(google_url: str) -> Optional[str]:
    """
    å¾ Google é‡å®šå‘ URL ä¸­æå–çœŸå¯¦çš„ç›®æ¨™ URL
    
    æ”¯æ´çš„æ ¼å¼ï¼š
    - Google News/Alerts: https://www.google.com/url?url=...
    - Google RSS: https://news.google.com/rss/articles/...
    
    Args:
        google_url: Google é‡å®šå‘ URL
        
    Returns:
        çœŸå¯¦çš„ç›®æ¨™ URLï¼Œå¦‚æœè§£æå¤±æ•—å‰‡è¿”å› None
        
    Examples:
        >>> decode_google_url('https://www.google.com/url?url=https://example.com&...')
        'https://example.com'
    """
    from urllib.parse import urlparse, parse_qs, unquote
    
    try:
        # è§£æ URL
        parsed = urlparse(google_url)
        
        # æª¢æŸ¥æ˜¯å¦ç‚º Google URL
        if 'google.com' not in parsed.netloc:
            # ä¸æ˜¯ Google URLï¼Œç›´æ¥è¿”å›åŸ URL
            return google_url
        
        # è§£ææŸ¥è©¢åƒæ•¸
        query_params = parse_qs(parsed.query)
        
        # å˜—è©¦å¾ä¸åŒçš„åƒæ•¸ä¸­æå– URL
        # å¸¸è¦‹åƒæ•¸ï¼šurl, q, u
        for param in ['url', 'q', 'u']:
            if param in query_params and query_params[param]:
                decoded_url = unquote(query_params[param][0])
                # ç¢ºä¿æ˜¯å®Œæ•´çš„ URL
                if decoded_url.startswith(('http://', 'https://')):
                    return decoded_url
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œè¿”å›åŸ URL
        return google_url
        
    except Exception as e:
        # è§£æå¤±æ•—ï¼Œè¿”å›åŸ URL
        return google_url


# é¦–é è·¯ç”±
@app.get("/")
async def root():
    """API é¦–é  - é¡¯ç¤ºå¯ç”¨ç«¯é»"""
    return {
        "message": "æ­¡è¿ä½¿ç”¨ç¶²é å…§å®¹è§£æå™¨ API (Python å¢å¼·ç‰ˆ)",
        "framework": "FastAPI + trafilatura + Playwright",
        "version": "1.4.0",
        "features": [
            "è‡ªå‹•é‡è©¦æ©Ÿåˆ¶ï¼ˆè™•ç† 403/429 éŒ¯èª¤ï¼‰",
            "éš¨æ©Ÿ User-Agent",
            "å¢å¼·çš„ HTTP headers",
            "SSL éŒ¯èª¤è™•ç†",
            "æŒ‡æ•¸é€€é¿ï¼ˆExponential Backoffï¼‰",
            "Playwright æ”¯æ´ï¼ˆè™•ç†å‹•æ…‹ JavaScript ç¶²ç«™ï¼‰",
            "å»£å‘Šå±è”½ï¼ˆNetwork å’Œ DOM å±¤é¢ï¼‰",
            "åçˆ¬èŸ²æ¨¡å¼ï¼ˆéš±è— webdriver ç‰¹å¾µï¼‰",
            "è‡ªå‹•æ»¾å‹•è¼‰å…¥æ‡¶åŠ è¼‰å…§å®¹"
        ],
        "endpoints": {
            "parse": {
                "method": "POST",
                "path": "/api/parse",
                "body": {
                    "url": "è¦è§£æçš„ç¶²é  URL",
                    "max_retries": "(é¸å¡«) æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼Œé è¨­ 3",
                    "skip_ssl": "(é¸å¡«) è·³é SSL é©—è­‰ï¼Œé è¨­ false"
                },
                "description": "è§£ææŒ‡å®š URL çš„ç¶²é å…§å®¹ï¼ˆåŒæ­¥å›å‚³ï¼Œæ”¯æ´é‡è©¦ï¼‰"
            },
            "parseGet": {
                "method": "GET",
                "path": "/api/parse?url=YOUR_URL",
                "description": "ä½¿ç”¨ GET æ–¹æ³•è§£æç¶²é å…§å®¹"
            },
            "parseDynamic": {
                "method": "POST",
                "path": "/api/parse-dynamic",
                "body": {
                    "url": "è¦è§£æçš„ç¶²é  URL",
                    "wait_for": "(é¸å¡«) ç­‰å¾…ç‰¹å®š CSS é¸æ“‡å™¨ï¼Œä¾‹å¦‚ 'article' æˆ– '.content'",
                    "block_ads": "(é¸å¡«) æ˜¯å¦å±è”½å»£å‘Šï¼Œé è¨­ true",
                    "stealth_mode": "(é¸å¡«) æ˜¯å¦å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼ï¼Œé è¨­ true"
                },
                "description": "ä½¿ç”¨ Playwright è§£æå‹•æ…‹ç¶²ç«™ï¼ˆæ”¯æ´ JavaScript æ¸²æŸ“ã€å»£å‘Šå±è”½ã€åçˆ¬èŸ²ï¼‰â­ æ¨è–¦ç”¨æ–¼ SPA ç¶²ç«™å’Œæœ‰åçˆ¬èŸ²çš„ç¶²ç«™"
            },
            "parseWebhook": {
                "method": "POST",
                "path": "/api/parse-webhook",
                "body": {
                    "url": "è¦è§£æçš„ç¶²é  URL",
                    "webhook_url": "n8n webhook URL",
                    "metadata": "(é¸å¡«) é¡å¤–è³‡æ–™",
                    "max_retries": "(é¸å¡«) æœ€å¤§é‡è©¦æ¬¡æ•¸",
                    "skip_ssl": "(é¸å¡«) è·³é SSL é©—è­‰"
                },
                "description": "è§£æç¶²é ä¸¦å›èª¿ webhookï¼ˆé©ç”¨æ–¼ n8n æ•´åˆï¼‰"
            },
            "decodeGoogleUrl": {
                "method": "POST",
                "path": "/api/decode-google-url",
                "body": {
                    "url": "Google é‡å®šå‘ URLï¼ˆä¾‹å¦‚ï¼šhttps://www.google.com/url?url=...ï¼‰"
                },
                "description": "å¾ Google é‡å®šå‘ URL ä¸­æå–çœŸå¯¦çš„ç›®æ¨™ URL â­ é©ç”¨æ–¼ Google Alert/RSS"
            },
            "decodeGoogleUrlGet": {
                "method": "GET",
                "path": "/api/decode-google-url?url=YOUR_GOOGLE_URL",
                "description": "ä½¿ç”¨ GET æ–¹æ³•è§£ç¢¼ Google URL"
            },
            "docs": {
                "method": "GET",
                "path": "/docs",
                "description": "Swagger UI äº’å‹•å¼ API æ–‡ä»¶"
            }
        },
        "examples": [
            'POST /api/parse with body: {"url": "https://example.com/article", "max_retries": 3}',
            'GET /api/parse?url=https://example.com/article',
            'POST /api/parse-webhook with body: {"url": "https://example.com/article", "webhook_url": "https://your-n8n.com/webhook/..."}',
            'POST /api/decode-google-url with body: {"url": "https://www.google.com/url?url=https://example.com/article&..."}',
            'GET /api/decode-google-url?url=https://www.google.com/url?url=https://example.com/article'
        ],
        "errorHandling": {
            "403 Forbidden": "è‡ªå‹•é‡è©¦ + éš¨æ©Ÿ User-Agent + Referer header",
            "429 Too Many Requests": "æŒ‡æ•¸é€€é¿é‡è©¦ï¼ˆ2s, 4s, 8s...ï¼‰",
            "SSL Certificate Error": "å¯é¸æ“‡è·³é SSL é©—è­‰ï¼ˆskip_ssl: trueï¼‰"
        },
        "documentation": "è¨ªå• /docs æŸ¥çœ‹å®Œæ•´ API æ–‡ä»¶"
    }


async def fetch_and_parse_with_retry(
    url: str, 
    max_retries: int = 3, 
    skip_ssl: bool = False
) -> Dict[str, Any]:
    """
    ä¸‹è¼‰ä¸¦è§£æç¶²é å…§å®¹ï¼ˆæ”¯æ´é‡è©¦ï¼‰
    
    Args:
        url: è¦è§£æçš„ç¶²é  URL
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        skip_ssl: æ˜¯å¦è·³é SSL é©—è­‰
        
    Returns:
        è§£æå¾Œçš„è³‡æ–™å­—å…¸
        
    Raises:
        HTTPException: ç•¶ä¸‹è¼‰æˆ–è§£æå¤±æ•—æ™‚
    """
    last_error = None
    
    for attempt in range(1, max_retries + 1):
        try:
            print(f"[å˜—è©¦ {attempt}/{max_retries}] è§£æ: {url}")
            
            # ç²å–å¢å¼·çš„ headers
            headers = get_enhanced_headers(url)
            
            # è¨­å®š timeout å’Œ SSL é©—è­‰
            timeout = httpx.Timeout(30.0, connect=10.0)
            verify_ssl = not skip_ssl
            
            # ä¸‹è¼‰ç¶²é å…§å®¹
            async with httpx.AsyncClient(
                timeout=timeout,
                verify=verify_ssl,
                follow_redirects=True,
                headers=headers
            ) as client:
                response = await client.get(url)
                response.raise_for_status()
                html_content = response.text
            
            # ä½¿ç”¨ trafilatura è§£æå…§å®¹
            try:
                text_content = trafilatura.extract(
                    html_content,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False
                )
            except Exception as e:
                print(f"[è­¦å‘Š] trafilatura.extract å¤±æ•—: {e}")
                text_content = None
            
            # æå–å®Œæ•´è³‡è¨Šï¼ˆåŒ…å«å…ƒæ•¸æ“šï¼‰
            try:
                metadata = trafilatura.extract_metadata(html_content)
            except Exception as e:
                print(f"[è­¦å‘Š] trafilatura.extract_metadata å¤±æ•—: {e}")
                metadata = None
            
            # æå– HTML æ ¼å¼çš„å…§å®¹
            try:
                html_formatted = trafilatura.extract(
                    html_content,
                    include_comments=False,
                    include_tables=True,
                    no_fallback=False,
                    output_format='xml'
                )
            except Exception as e:
                print(f"[è­¦å‘Š] trafilatura.extract (XML) å¤±æ•—: {e}")
                html_formatted = None
            
            # æ•´ç†å›å‚³è³‡æ–™
            parsed_data = {
                "title": getattr(metadata, 'title', None) if metadata else None,
                "author": getattr(metadata, 'author', None) if metadata else None,
                "date_published": getattr(metadata, 'date', None) if metadata else None,
                "url": getattr(metadata, 'url', url) if metadata else url,
                "domain": getattr(metadata, 'sitename', None) if metadata else None,
                "description": getattr(metadata, 'description', None) if metadata else None,
                "categories": getattr(metadata, 'categories', None) if metadata else None,
                "tags": getattr(metadata, 'tags', None) if metadata else None,
                "content": html_formatted or text_content,
                "text_content": text_content,
                "excerpt": text_content[:200] + "..." if text_content and len(text_content) > 200 else text_content,
                "word_count": len(text_content.split()) if text_content else 0,
                "language": getattr(metadata, 'language', None) if metadata else None
            }
            
            title_preview = parsed_data.get('title') or 'No title'
            print(f"[æˆåŠŸ] å˜—è©¦ {attempt}: {title_preview[:50] if title_preview else 'No title'}")
            return {
                "success": True,
                "data": parsed_data,
                "attempt": attempt,
                "retries": attempt - 1
            }
            
        except httpx.HTTPStatusError as e:
            last_error = e
            status_code = e.response.status_code
            print(f"[å¤±æ•—] å˜—è©¦ {attempt}: HTTP {status_code} - {str(e)}")
            
            # å¦‚æœæ˜¯æœ€å¾Œä¸€æ¬¡å˜—è©¦ï¼Œæ‹‹å‡ºéŒ¯èª¤
            if attempt == max_retries:
                raise HTTPException(
                    status_code=500,
                    detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: Client error '{status_code} {e.response.reason_phrase}' for url '{url}'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/{status_code}"
                )
            
            # æ ¹æ“šéŒ¯èª¤é¡å‹æ±ºå®šç­‰å¾…æ™‚é–“
            if status_code in [429, 403]:
                # 429 Too Many Requests æˆ– 403 Forbiddenï¼šæŒ‡æ•¸é€€é¿
                wait_time = (2 ** attempt)  # 2ç§’ã€4ç§’ã€8ç§’...
                print(f"[ç­‰å¾…] {wait_time} ç§’å¾Œé‡è©¦ï¼ˆHTTP {status_code}ï¼‰...")
                await asyncio.sleep(wait_time)
            else:
                # å…¶ä»–éŒ¯èª¤ï¼šçŸ­æš«ç­‰å¾…
                await asyncio.sleep(1)
                
        except httpx.ConnectError as e:
            last_error = e
            print(f"[å¤±æ•—] å˜—è©¦ {attempt}: é€£æ¥éŒ¯èª¤ - {str(e)}")
            
            if attempt == max_retries:
                raise HTTPException(
                    status_code=500,
                    detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: ç„¡æ³•é€£æ¥åˆ° {url}"
                )
            
            await asyncio.sleep(2)
            
        except Exception as e:
            error_msg = str(e)
            print(f"[å¤±æ•—] å˜—è©¦ {attempt}: {error_msg}")
            
            # SSL éŒ¯èª¤è™•ç†
            if "SSL" in error_msg or "certificate" in error_msg.lower():
                last_error = e
                
                if attempt == max_retries:
                    if skip_ssl:
                        raise HTTPException(
                            status_code=500,
                            detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: {error_msg}"
                        )
                    else:
                        raise HTTPException(
                            status_code=500,
                            detail=f"ä¸‹è¼‰ç¶²é å¤±æ•—: {error_msg}\n\nğŸ’¡ æç¤ºï¼šå¯ä»¥å˜—è©¦è¨­å®š skip_ssl: true ä¾†è·³é SSL é©—è­‰"
                        )
                
                # ä¸‹æ¬¡å˜—è©¦æ™‚è·³é SSL é©—è­‰
                if not skip_ssl:
                    print(f"[SSL éŒ¯èª¤] ä¸‹æ¬¡å°‡è·³é SSL é©—è­‰...")
                    skip_ssl = True
                    
                await asyncio.sleep(1)
            else:
                # å…¶ä»–éŒ¯èª¤
                last_error = e
                
                if attempt == max_retries:
                    raise HTTPException(
                        status_code=500,
                        detail=f"è§£æç¶²é å¤±æ•—: {error_msg}"
                    )
                
                await asyncio.sleep(1)
    
    # ç†è«–ä¸Šä¸æœƒåˆ°é”é€™è£¡ï¼Œä½†ä»¥é˜²è¬ä¸€
    raise HTTPException(
        status_code=500,
        detail=f"è§£æç¶²é å¤±æ•—: {str(last_error)}"
    )


@app.post("/api/parse")
async def parse_url(request: ParseRequest):
    """
    POST æ–¹æ³•ï¼šè§£æç¶²é å…§å®¹ï¼ˆæ”¯æ´é‡è©¦ï¼‰
    
    Args:
        request: åŒ…å« urlã€max_retries å’Œ skip_ssl çš„è«‹æ±‚ç‰©ä»¶
        
    Returns:
        è§£æå¾Œçš„ç¶²é å…§å®¹
    """
    print(f"æ­£åœ¨è§£æ: {request.url} (max_retries: {request.max_retries}, skip_ssl: {request.skip_ssl})")
    
    try:
        result = await fetch_and_parse_with_retry(
            request.url,
            max_retries=request.max_retries,
            skip_ssl=request.skip_ssl
        )
        
        return result
        
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"è§£æéŒ¯èª¤: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"è§£æç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


@app.get("/api/parse")
async def parse_url_get(url: str, max_retries: int = 3, skip_ssl: bool = False):
    """
    GET æ–¹æ³•ï¼šè§£æç¶²é å…§å®¹ï¼ˆé€é query stringï¼‰
    
    Args:
        url: è¦è§£æçš„ç¶²é  URL
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸ï¼ˆé è¨­ 3ï¼‰
        skip_ssl: æ˜¯å¦è·³é SSL é©—è­‰ï¼ˆé è¨­ Falseï¼‰
        
    Returns:
        è§£æå¾Œçš„ç¶²é å…§å®¹
    """
    if not url:
        raise HTTPException(
            status_code=400,
            detail="è«‹åœ¨ URL åƒæ•¸ä¸­æä¾›è¦è§£æçš„ç¶²å€"
        )
    
    print(f"æ­£åœ¨è§£æ (GET): {url}")
    
    try:
        result = await fetch_and_parse_with_retry(
            url,
            max_retries=max_retries,
            skip_ssl=skip_ssl
        )
        
        return result
        
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"è§£æéŒ¯èª¤: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"è§£æç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


@app.post("/api/parse-dynamic")
async def parse_url_dynamic(request: ParseDynamicRequest):
    """
    POST æ–¹æ³•ï¼šä½¿ç”¨ Playwright è§£æå‹•æ…‹ç¶²ç«™ï¼ˆå¢å¼·ç‰ˆï¼šæ”¯æ´å»£å‘Šå±è”½å’Œåçˆ¬èŸ²ï¼‰
    
    é©ç”¨æ–¼ï¼š
    - React/Vue/Angular ç­‰å–®é æ‡‰ç”¨ï¼ˆSPAï¼‰
    - JavaScript å‹•æ…‹è¼‰å…¥å…§å®¹çš„ç¶²ç«™
    - éœ€è¦ç­‰å¾…ç‰¹å®šå…ƒç´ å‡ºç¾çš„ç¶²ç«™
    - æœ‰å»£å‘Šå¹²æ“¾çš„ç¶²ç«™
    - æœ‰åçˆ¬èŸ²æ©Ÿåˆ¶çš„ç¶²ç«™
    
    Args:
        request: åŒ…å«ä»¥ä¸‹æ¬„ä½çš„è«‹æ±‚ç‰©ä»¶
            - url: è¦è§£æçš„ç¶²é  URL
            - wait_for: (é¸å¡«) ç­‰å¾…ç‰¹å®š CSS é¸æ“‡å™¨
            - block_ads: (é¸å¡«) æ˜¯å¦å±è”½å»£å‘Šï¼Œé è¨­ True
            - stealth_mode: (é¸å¡«) æ˜¯å¦å•Ÿç”¨åçˆ¬èŸ²æ¨¡å¼ï¼Œé è¨­ True
        
    Returns:
        è§£æå¾Œçš„ç¶²é å…§å®¹
        
    Example:
        POST /api/parse-dynamic
        {
            "url": "https://applealmond.com/posts/296254",
            "wait_for": ".post-content",
            "block_ads": true,
            "stealth_mode": true
        }
    """
    print(f"æ­£åœ¨ä½¿ç”¨ Playwright è§£æ: {request.url}")
    print(f"å»£å‘Šå±è”½: {request.block_ads}, åçˆ¬èŸ²æ¨¡å¼: {request.stealth_mode}")
    if request.wait_for:
        print(f"ç­‰å¾…å…ƒç´ : {request.wait_for}")
    
    try:
        result = await fetch_and_parse_with_playwright(
            request.url, 
            request.wait_for,
            request.block_ads,
            request.stealth_mode
        )
        return result
        
    except HTTPException as e:
        raise e
    except Exception as e:
        print(f"Playwright è§£æéŒ¯èª¤: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"ä½¿ç”¨ Playwright è§£æç¶²é æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


async def process_and_webhook(
    url: str, 
    webhook_url: str, 
    metadata: Dict[str, Any],
    max_retries: int = 3,
    skip_ssl: bool = False
):
    """
    èƒŒæ™¯ä»»å‹™ï¼šè§£æç¶²é ä¸¦å›èª¿ webhook
    
    Args:
        url: è¦è§£æçš„ç¶²é  URL
        webhook_url: webhook å›èª¿ URL
        metadata: é¡å¤–çš„å…ƒæ•¸æ“š
        max_retries: æœ€å¤§é‡è©¦æ¬¡æ•¸
        skip_ssl: æ˜¯å¦è·³é SSL é©—è­‰
    """
    print(f"æ­£åœ¨è§£æ (webhook æ¨¡å¼): {url}")
    
    try:
        # è§£æç¶²é ï¼ˆä½¿ç”¨é‡è©¦æ©Ÿåˆ¶ï¼‰
        result = await fetch_and_parse_with_retry(url, max_retries, skip_ssl)
        
        # æº–å‚™å›èª¿è³‡æ–™
        webhook_data = {
            "success": True,
            "original_url": url,
            "metadata": metadata,
            "parsed_data": result.get("data"),
            "attempt": result.get("attempt"),
            "retries": result.get("retries"),
            "parsed_at": datetime.now().isoformat()
        }
        
        # å›èª¿ webhook
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(
                webhook_url,
                json=webhook_data
            )
            
            if response.status_code == 200:
                print(f"âœ… Webhook å›èª¿æˆåŠŸ: {webhook_url}")
            else:
                print(f"âŒ Webhook å›èª¿å¤±æ•— ({response.status_code}): {webhook_url}")
                
    except Exception as e:
        print(f"è§£ææˆ–å›èª¿éŒ¯èª¤: {str(e)}")
        
        # å˜—è©¦å›èª¿éŒ¯èª¤è¨Šæ¯
        try:
            error_data = {
                "success": False,
                "original_url": url,
                "metadata": metadata,
                "error": str(e),
                "failed_at": datetime.now().isoformat()
            }
            
            async with httpx.AsyncClient(timeout=30.0) as client:
                await client.post(webhook_url, json=error_data)
                
        except Exception as webhook_error:
            print(f"ç„¡æ³•å›èª¿éŒ¯èª¤è¨Šæ¯: {str(webhook_error)}")


@app.post("/api/parse-webhook")
async def parse_url_webhook(request: ParseWebhookRequest, background_tasks: BackgroundTasks):
    """
    POST æ–¹æ³•ï¼šè§£æç¶²é ä¸¦å›èª¿ webhookï¼ˆç”¨æ–¼ n8n æ•´åˆï¼‰
    
    Args:
        request: åŒ…å« urlã€webhook_urlã€metadataã€max_retries å’Œ skip_ssl çš„è«‹æ±‚ç‰©ä»¶
        background_tasks: FastAPI èƒŒæ™¯ä»»å‹™ç®¡ç†å™¨
        
    Returns:
        ä»»å‹™æ¥æ”¶ç¢ºèª
    """
    # åŠ å…¥èƒŒæ™¯ä»»å‹™
    background_tasks.add_task(
        process_and_webhook,
        request.url,
        request.webhook_url,
        request.metadata,
        request.max_retries,
        request.skip_ssl
    )
    
    return {
        "success": True,
        "message": "è§£æä»»å‹™å·²æ¥æ”¶ï¼Œå°‡åœ¨å®Œæˆå¾Œå›èª¿ webhook",
        "url": request.url,
        "webhook_url": request.webhook_url,
        "max_retries": request.max_retries
    }


@app.post("/api/decode-google-url")
async def decode_google_url_post(request: DecodeGoogleUrlRequest):
    """
    POST æ–¹æ³•ï¼šå¾ Google é‡å®šå‘ URL ä¸­æå–çœŸå¯¦çš„ç›®æ¨™ URL
    
    æ”¯æ´çš„æ ¼å¼ï¼š
    - Google News/Alerts: https://www.google.com/url?url=...
    - Google RSS: https://news.google.com/rss/articles/...
    
    Args:
        request: åŒ…å« url çš„è«‹æ±‚ç‰©ä»¶
        
    Returns:
        åŒ…å«åŸå§‹ URL å’Œè§£ç¢¼å¾Œ URL çš„ JSON å›æ‡‰
        
    Example:
        POST /api/decode-google-url
        {
            "url": "https://www.google.com/url?url=https://example.com/article&..."
        }
        
        Response:
        {
            "success": true,
            "original_url": "https://www.google.com/url?url=...",
            "decoded_url": "https://example.com/article",
            "is_google_url": true
        }
    """
    try:
        original_url = request.url
        decoded_url = decode_google_url(original_url)
        
        # æª¢æŸ¥æ˜¯å¦ç‚º Google URL
        is_google_url = 'google.com' in original_url
        
        return {
            "success": True,
            "original_url": original_url,
            "decoded_url": decoded_url,
            "is_google_url": is_google_url,
            "changed": original_url != decoded_url
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è§£ç¢¼ URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


@app.get("/api/decode-google-url")
async def decode_google_url_get(url: str):
    """
    GET æ–¹æ³•ï¼šå¾ Google é‡å®šå‘ URL ä¸­æå–çœŸå¯¦çš„ç›®æ¨™ URL
    
    Args:
        url: Google é‡å®šå‘ URLï¼ˆä½œç‚ºæŸ¥è©¢åƒæ•¸ï¼‰
        
    Returns:
        åŒ…å«åŸå§‹ URL å’Œè§£ç¢¼å¾Œ URL çš„ JSON å›æ‡‰
        
    Example:
        GET /api/decode-google-url?url=https://www.google.com/url?url=https://example.com/article
        
        Response:
        {
            "success": true,
            "original_url": "https://www.google.com/url?url=...",
            "decoded_url": "https://example.com/article",
            "is_google_url": true
        }
    """
    if not url:
        raise HTTPException(
            status_code=400,
            detail="è«‹æä¾› URL åƒæ•¸"
        )
    
    try:
        decoded_url = decode_google_url(url)
        
        # æª¢æŸ¥æ˜¯å¦ç‚º Google URL
        is_google_url = 'google.com' in url
        
        return {
            "success": True,
            "original_url": url,
            "decoded_url": decoded_url,
            "is_google_url": is_google_url,
            "changed": url != decoded_url
        }
        
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"è§£ç¢¼ URL æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
        )


@app.get("/health")
async def health_check():
    """å¥åº·æª¢æŸ¥ç«¯é»"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "service": "parser-api",
        "version": "1.4.0",
        "features": [
            "retry-mechanism",
            "enhanced-headers",
            "ssl-handling",
            "exponential-backoff",
            "playwright-dynamic-rendering",
            "ad-blocking",
            "anti-bot-detection",
            "lazy-loading-support"
        ]
    }


if __name__ == "__main__":
    # å¾ç’°å¢ƒè®Šæ•¸è®€å–åŸ è™Ÿï¼ˆRailway æœƒæä¾›ï¼‰ï¼Œé è¨­ 3000
    port = int(os.getenv("PORT", 3000))
    
    print("ğŸš€ Parser ä¼ºæœå™¨å·²å•Ÿå‹•ï¼ï¼ˆPython å¢å¼·ç‰ˆ v1.2.0ï¼‰")
    print(f"ğŸ“¡ ç›£è½åŸ è™Ÿ: {port}")
    print(f"ğŸŒ æœ¬åœ°è¨ªå•: http://localhost:{port}")
    print(f"ğŸ“š API æ–‡ä»¶: http://localhost:{port}/docs")
    print("\nğŸ›¡ï¸  å¢å¼·åŠŸèƒ½:")
    print("  âœ“ è‡ªå‹•é‡è©¦æ©Ÿåˆ¶ï¼ˆ403/429 éŒ¯èª¤ï¼‰")
    print("  âœ“ éš¨æ©Ÿ User-Agent")
    print("  âœ“ SSL éŒ¯èª¤è™•ç†")
    print("  âœ“ æŒ‡æ•¸é€€é¿é‡è©¦")
    print("\nä½¿ç”¨ç¯„ä¾‹:")
    print(f"  POST http://localhost:{port}/api/parse")
    print('  Body: {"url": "https://example.com/article", "max_retries": 3}')
    print("\n  æˆ–ä½¿ç”¨ GET:")
    print(f"  http://localhost:{port}/api/parse?url=https://example.com/article")
    print("\næŒ‰ Ctrl+C åœæ­¢ä¼ºæœå™¨\n")
    
    # å•Ÿå‹•ä¼ºæœå™¨
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )
